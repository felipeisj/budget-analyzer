{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbf296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuraci√≥n completa\n",
      "üìÅ Directorio bases: storage/projects/conservacion_caminos/bases\n",
      "üìÅ Directorio resultados: storage/projects/conservacion_caminos/results\n",
      "üìÅ Directorio temporal: storage/projects/conservacion_caminos/temp\n",
      "üîë API Keys configuradas\n",
      "üöÄ Listo para procesamiento paralelo\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 1: CONFIGURACI√ìN E IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configuraci√≥n\n",
    "load_dotenv(Path('.env') if Path('.env').exists() else Path('../.env'))\n",
    "\n",
    "# Verificar API keys\n",
    "PDF_REST_API_KEY = os.getenv('PDF_REST_API_KEY')\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if not PDF_REST_API_KEY:\n",
    "    raise ValueError(\"‚ùå PDF_REST_API_KEY no encontrada en .env\")\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    raise ValueError(\"‚ùå ANTHROPIC_API_KEY no encontrada en .env\")\n",
    "\n",
    "# Configuraci√≥n de rutas\n",
    "BASE_DIR = Path(\"storage/projects/conservacion_caminos\")\n",
    "BASES_DIR = BASE_DIR / \"bases\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "TEMP_DIR = BASE_DIR / \"temp\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Inicializar cliente Anthropic\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n completa\")\n",
    "print(f\"üìÅ Directorio bases: {BASES_DIR}\")\n",
    "print(f\"üìÅ Directorio resultados: {RESULTS_DIR}\")\n",
    "print(f\"üìÅ Directorio temporal: {TEMP_DIR}\")\n",
    "print(f\"üîë API Keys configuradas\")\n",
    "print(f\"üöÄ Listo para procesamiento paralelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34110b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuraci√≥n adicional cargada\n",
      "   - tqdm importado para barras de progreso\n",
      "   - CONFIG definido con par√°metros del sistema\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 1-B: AGREGAR IMPORTS FALTANTES (ejecutar despu√©s de CELDA 1)\n",
    "# ============================================================================\n",
    "\n",
    "# Imports adicionales necesarios\n",
    "from tqdm.notebook import tqdm  # Para barras de progreso en Jupyter\n",
    "\n",
    "# Configuraci√≥n global que faltaba\n",
    "CONFIG = {\n",
    "    'CHUNK_SIZE': 15,  # P√°ginas por chunk\n",
    "    'MAX_WORKERS': 4,   # Workers paralelos\n",
    "    'OCR_TIMEOUT': 600, # Timeout para OCR (10 minutos)\n",
    "    'MAX_TEXT_FOR_AI': 80000,  # Caracteres m√°ximos para Claude\n",
    "    'MIN_VALID_TEXT': 1000,    # M√≠nimo de caracteres para considerar v√°lido\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n adicional cargada\")\n",
    "print(f\"   - tqdm importado para barras de progreso\")\n",
    "print(f\"   - CONFIG definido con par√°metros del sistema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4eb9490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PDFSplitter inicializado (30 p√°ginas por chunk)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 2: CLASE PARA DIVISI√ìN DE PDFs\n",
    "# ============================================================================\n",
    "\n",
    "class PDFSplitter:\n",
    "    \"\"\"Divide PDFs grandes en chunks para procesamiento paralelo.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_pages_per_chunk: int = 30):\n",
    "        self.max_pages_per_chunk = max_pages_per_chunk\n",
    "    \n",
    "    def split_pdf(self, pdf_path: Path, output_dir: Path = None) -> List[Tuple[Path, int, int]]:\n",
    "        \"\"\"\n",
    "        Divide PDF en chunks y retorna lista de (chunk_path, start_page, end_page).\n",
    "        \"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = TEMP_DIR / f\"{pdf_path.stem}_chunks\"\n",
    "        \n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nüìÑ Dividiendo PDF: {pdf_path.name}\")\n",
    "        print(f\"   üìè Tama√±o: {pdf_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "        \n",
    "        # Obtener total de p√°ginas\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            total_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        print(f\"   üìë Total p√°ginas: {total_pages}\")\n",
    "        \n",
    "        chunks_info = []\n",
    "        chunk_number = 1\n",
    "        \n",
    "        for start_page in range(0, total_pages, self.max_pages_per_chunk):\n",
    "            end_page = min(start_page + self.max_pages_per_chunk, total_pages)\n",
    "            \n",
    "            # Crear nombre del chunk\n",
    "            chunk_filename = f\"{pdf_path.stem}_chunk_{chunk_number:02d}_p{start_page+1}-{end_page}.pdf\"\n",
    "            chunk_path = output_dir / chunk_filename\n",
    "            \n",
    "            # Escribir chunk\n",
    "            with open(pdf_path, 'rb') as input_file:\n",
    "                pdf_reader = PyPDF2.PdfReader(input_file)\n",
    "                pdf_writer = PyPDF2.PdfWriter()\n",
    "                \n",
    "                for page_num in range(start_page, end_page):\n",
    "                    pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "                \n",
    "                with open(chunk_path, 'wb') as output_file:\n",
    "                    pdf_writer.write(output_file)\n",
    "            \n",
    "            chunks_info.append((chunk_path, start_page + 1, end_page))\n",
    "            print(f\"   ‚úÖ Chunk {chunk_number}: p√°ginas {start_page+1}-{end_page}\")\n",
    "            chunk_number += 1\n",
    "        \n",
    "        print(f\"   üì¶ Total chunks creados: {len(chunks_info)}\")\n",
    "        return chunks_info\n",
    "    \n",
    "    def cleanup_chunks(self, chunks_dir: Path):\n",
    "        \"\"\"Limpia los archivos temporales de chunks.\"\"\"\n",
    "        try:\n",
    "            if chunks_dir.exists():\n",
    "                for file in chunks_dir.glob(\"*.pdf\"):\n",
    "                    file.unlink()\n",
    "                chunks_dir.rmdir()\n",
    "                print(f\"   üßπ Limpieza completada: {chunks_dir.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error limpiando chunks: {e}\")\n",
    "\n",
    "# Inicializar splitter\n",
    "splitter = PDFSplitter(max_pages_per_chunk=30)\n",
    "print(\"‚úÖ PDFSplitter inicializado (30 p√°ginas por chunk)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b275ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 3: FUNCIONES OCR MEJORADAS - PARALELO REAL Y MAYOR EXTRACCI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "def apply_ocr_enhanced(chunk_path: Path, api_key: str, \n",
    "                       timeout: int = 600, retry_count: int = 2) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    OCR mejorado con reintentos y mejor extracci√≥n de texto.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"chunk_name\": chunk_path.name,\n",
    "        \"success\": False,\n",
    "        \"text\": \"\",\n",
    "        \"error\": None,\n",
    "        \"processing_time\": 0,\n",
    "        \"characters\": 0,\n",
    "        \"retry_attempts\": 0\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for attempt in range(retry_count + 1):\n",
    "        try:\n",
    "            print(f\"      üîÑ Intento {attempt + 1} para {chunk_path.name}\")\n",
    "            \n",
    "            # PASO 1: Aplicar OCR\n",
    "            ocr_url = \"https://api.pdfrest.com/pdf-with-ocr-text\"\n",
    "            \n",
    "            with open(chunk_path, 'rb') as file:\n",
    "                files = [('file', (chunk_path.name, file, 'application/pdf'))]\n",
    "                headers = {'Api-Key': api_key}\n",
    "                payload = {\n",
    "                    'output': f'ocr_{chunk_path.stem}',\n",
    "                    'languages': 'Spanish,English',  # A√±adido ingl√©s tambi√©n\n",
    "                    'ocr_library': 'tesseract'      # Especificar librer√≠a\n",
    "                }\n",
    "                \n",
    "                response = requests.post(\n",
    "                    ocr_url,\n",
    "                    headers=headers,\n",
    "                    data=payload,\n",
    "                    files=files,\n",
    "                    timeout=timeout\n",
    "                )\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                raise Exception(f\"OCR failed: HTTP {response.status_code}\")\n",
    "            \n",
    "            # Obtener URL del PDF procesado\n",
    "            data = response.json()\n",
    "            output_url = data.get('outputUrl')\n",
    "            \n",
    "            if not output_url:\n",
    "                raise Exception(\"No output URL from OCR\")\n",
    "            \n",
    "            # PASO 2: Descargar PDF procesado\n",
    "            pdf_response = requests.get(output_url, timeout=120)\n",
    "            if pdf_response.status_code != 200:\n",
    "                raise Exception(f\"Download failed: HTTP {pdf_response.status_code}\")\n",
    "            \n",
    "            # PASO 3: Guardar temporalmente y extraer texto\n",
    "            temp_pdf = TEMP_DIR / f\"temp_ocr_{chunk_path.stem}_{attempt}.pdf\"\n",
    "            \n",
    "            with open(temp_pdf, 'wb') as f:\n",
    "                f.write(pdf_response.content)\n",
    "            \n",
    "            # PASO 4: Extraer texto con m√∫ltiples m√©todos\n",
    "            extracted_text = \"\"\n",
    "            \n",
    "            # M√©todo 1: API pdfRest para extracci√≥n\n",
    "            extract_url = \"https://api.pdfrest.com/extracted-text\"\n",
    "            with open(temp_pdf, 'rb') as file:\n",
    "                files = [('file', (temp_pdf.name, file, 'application/pdf'))]\n",
    "                headers = {'Api-Key': api_key}\n",
    "                response = requests.post(extract_url, headers=headers, files=files, timeout=120)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    extracted_text = response.json().get('fullText', '')\n",
    "            \n",
    "            # M√©todo 2: Si falla o est√° vac√≠o, usar PyPDF2 como respaldo\n",
    "            if len(extracted_text) < 100:\n",
    "                try:\n",
    "                    with open(temp_pdf, 'rb') as f:\n",
    "                        reader = PyPDF2.PdfReader(f)\n",
    "                        backup_text = \"\"\n",
    "                        for page in reader.pages:\n",
    "                            backup_text += page.extract_text()\n",
    "                        \n",
    "                        if len(backup_text) > len(extracted_text):\n",
    "                            extracted_text = backup_text\n",
    "                            print(f\"         üìã Usando PyPDF2 como respaldo ({len(backup_text)} chars)\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Limpiar watermarks y texto basura\n",
    "            extracted_text = re.sub(r'\\[pdfRest.*?\\]', '', extracted_text)\n",
    "            extracted_text = re.sub(r'Page \\d+ of \\d+', '', extracted_text)\n",
    "            extracted_text = re.sub(r'\\n{3,}', '\\n\\n', extracted_text)\n",
    "            \n",
    "            # Limpiar archivo temporal\n",
    "            if temp_pdf.exists():\n",
    "                temp_pdf.unlink()\n",
    "            \n",
    "            if len(extracted_text) > 50:  # M√≠nimo 50 caracteres para considerarlo v√°lido\n",
    "                result[\"success\"] = True\n",
    "                result[\"text\"] = extracted_text\n",
    "                result[\"characters\"] = len(extracted_text)\n",
    "                result[\"retry_attempts\"] = attempt\n",
    "                print(f\"      ‚úÖ √âxito: {len(extracted_text):,} caracteres extra√≠dos\")\n",
    "                break\n",
    "            else:\n",
    "                raise Exception(f\"Texto insuficiente: solo {len(extracted_text)} caracteres\")\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            result[\"error\"] = f\"Timeout en intento {attempt + 1}\"\n",
    "            print(f\"      ‚è±Ô∏è Timeout en intento {attempt + 1}\")\n",
    "        except Exception as e:\n",
    "            result[\"error\"] = str(e)\n",
    "            print(f\"      ‚ùå Error en intento {attempt + 1}: {str(e)[:50]}\")\n",
    "        \n",
    "        if attempt < retry_count:\n",
    "            time.sleep(2)  # Esperar antes de reintentar\n",
    "    \n",
    "    result[\"processing_time\"] = time.time() - start_time\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_chunks_parallel_enhanced(chunks_info: List[Tuple[Path, int, int]], \n",
    "                                    api_key: str,\n",
    "                                    max_workers: int = 4) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Procesamiento paralelo real y optimizado de chunks.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ PROCESAMIENTO PARALELO OPTIMIZADO\")\n",
    "    print(f\"   üì¶ Chunks a procesar: {len(chunks_info)}\")\n",
    "    print(f\"   üë∑ Workers paralelos: {max_workers}\")\n",
    "    print(\"   \" + \"=\"*60)\n",
    "    \n",
    "    results = []\n",
    "    all_texts = {}  # Diccionario para mantener orden de p√°ginas\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Crear futures para todos los chunks\n",
    "        future_to_chunk = {\n",
    "            executor.submit(apply_ocr_enhanced, chunk_path, api_key): (chunk_path, start, end)\n",
    "            for chunk_path, start, end in chunks_info\n",
    "        }\n",
    "        \n",
    "        # Procesar resultados conforme se completan\n",
    "        with tqdm(total=len(chunks_info), desc=\"   Procesando chunks\") as pbar:\n",
    "            for future in as_completed(future_to_chunk):\n",
    "                chunk_path, start_page, end_page = future_to_chunk[future]\n",
    "                \n",
    "                try:\n",
    "                    result = future.result(timeout=700)  # Timeout para el future\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    if result[\"success\"]:\n",
    "                        all_texts[start_page] = {\n",
    "                            'pages': (start_page, end_page),\n",
    "                            'text': result['text'],\n",
    "                            'chars': result['characters']\n",
    "                        }\n",
    "                        successful += 1\n",
    "                        total_chars += result['characters']\n",
    "                        print(f\"   ‚úÖ P√°ginas {start_page:3d}-{end_page:3d}: {result['characters']:,} chars\")\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                        print(f\"   ‚ùå P√°ginas {start_page:3d}-{end_page:3d}: {result['error'][:50]}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    failed += 1\n",
    "                    print(f\"   ‚ùå P√°ginas {start_page:3d}-{end_page:3d}: Error cr√≠tico: {str(e)[:50]}\")\n",
    "                    results.append({\n",
    "                        \"chunk_name\": chunk_path.name,\n",
    "                        \"success\": False,\n",
    "                        \"error\": str(e)\n",
    "                    })\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Consolidar texto en orden correcto de p√°ginas\n",
    "    sorted_pages = sorted(all_texts.keys())\n",
    "    consolidated_text = \"\"\n",
    "    \n",
    "    for page_num in sorted_pages:\n",
    "        page_data = all_texts[page_num]\n",
    "        consolidated_text += f\"\\n\\n{'='*80}\\n\"\n",
    "        consolidated_text += f\"P√ÅGINAS {page_data['pages'][0]}-{page_data['pages'][1]}\\n\"\n",
    "        consolidated_text += f\"{'='*80}\\n\\n\"\n",
    "        consolidated_text += page_data['text']\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Estad√≠sticas finales\n",
    "    print(f\"\\n   üìä ESTAD√çSTICAS DEL PROCESAMIENTO:\")\n",
    "    print(f\"      ‚úÖ Chunks exitosos: {successful}/{len(chunks_info)}\")\n",
    "    print(f\"      ‚ùå Chunks fallidos: {failed}/{len(chunks_info)}\")\n",
    "    print(f\"      üìù Total caracteres: {total_chars:,}\")\n",
    "    print(f\"      ‚è±Ô∏è Tiempo total: {elapsed_time:.1f}s\")\n",
    "    print(f\"      ‚ö° Velocidad: {elapsed_time/len(chunks_info):.1f}s por chunk\")\n",
    "    \n",
    "    return {\n",
    "        \"success\": successful > 0,\n",
    "        \"text\": consolidated_text,\n",
    "        \"chunks_processed\": len(chunks_info),\n",
    "        \"chunks_successful\": successful,\n",
    "        \"chunks_failed\": failed,\n",
    "        \"total_characters\": len(consolidated_text),\n",
    "        \"total_processing_time\": elapsed_time,\n",
    "        \"avg_time_per_chunk\": elapsed_time / len(chunks_info) if chunks_info else 0,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funciones OCR optimizadas cargadas\")\n",
    "print(\"   - Reintentos autom√°ticos\")\n",
    "print(\"   - Procesamiento paralelo real\")\n",
    "print(\"   - M√∫ltiples m√©todos de extracci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ca9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 3-B: FUNCI√ìN process_chunks_parallel ORIGINAL (que faltaba)\n",
    "# ============================================================================\n",
    "\n",
    "def process_chunks_parallel(chunks_info: List[Tuple[Path, int, int]], \n",
    "                          api_key: str,\n",
    "                          max_workers: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Procesa m√∫ltiples chunks en paralelo - versi√≥n compatible.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Procesando {len(chunks_info)} chunks en paralelo (max {max_workers} workers)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    all_texts = []\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Enviar todos los chunks a procesar\n",
    "        future_to_chunk = {\n",
    "            executor.submit(apply_ocr_to_chunk, chunk_path, api_key): (chunk_path, start, end)\n",
    "            for chunk_path, start, end in chunks_info\n",
    "        }\n",
    "        \n",
    "        # Procesar resultados conforme se completan\n",
    "        for future in as_completed(future_to_chunk):\n",
    "            chunk_path, start_page, end_page = future_to_chunk[future]\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "                if result[\"success\"]:\n",
    "                    all_texts.append(f\"\\n--- P√°ginas {start_page}-{end_page} ---\\n{result['text']}\")\n",
    "                    successful += 1\n",
    "                    print(f\"   ‚úÖ {result['chunk_name']}: {result['characters']:,} chars en {result['processing_time']:.1f}s\")\n",
    "                else:\n",
    "                    failed += 1\n",
    "                    print(f\"   ‚ùå {result['chunk_name']}: {result['error']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                print(f\"   ‚ùå {chunk_path.name}: Error inesperado: {e}\")\n",
    "                results.append({\n",
    "                    \"chunk_name\": chunk_path.name,\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "    \n",
    "    # Consolidar texto\n",
    "    consolidated_text = \"\\n\".join(all_texts)\n",
    "    \n",
    "    total_time = sum(r.get(\"processing_time\", 0) for r in results)\n",
    "    \n",
    "    return {\n",
    "        \"success\": successful > 0,\n",
    "        \"text\": consolidated_text,\n",
    "        \"chunks_processed\": len(chunks_info),\n",
    "        \"chunks_successful\": successful,\n",
    "        \"chunks_failed\": failed,\n",
    "        \"total_characters\": len(consolidated_text),\n",
    "        \"total_processing_time\": total_time,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ process_chunks_parallel() definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 3-C: FUNCI√ìN apply_ocr_to_chunk ORIGINAL (que faltaba)\n",
    "# ============================================================================\n",
    "\n",
    "def apply_ocr_to_chunk(chunk_path: Path, api_key: str, timeout: int = 300) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Aplica OCR a un chunk individual usando pdfRest.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"chunk_name\": chunk_path.name,\n",
    "        \"success\": False,\n",
    "        \"text\": \"\",\n",
    "        \"error\": None,\n",
    "        \"processing_time\": 0,\n",
    "        \"characters\": 0\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Paso 1: Aplicar OCR al PDF\n",
    "        ocr_url = \"https://api.pdfrest.com/pdf-with-ocr-text\"\n",
    "        \n",
    "        with open(chunk_path, 'rb') as file:\n",
    "            payload = {\n",
    "                'output': f'ocr_{chunk_path.stem}',\n",
    "                'languages': 'Spanish'\n",
    "            }\n",
    "            files = [('file', (chunk_path.name, file, 'application/pdf'))]\n",
    "            headers = {'Api-Key': api_key}\n",
    "            \n",
    "            response = requests.post(\n",
    "                ocr_url,\n",
    "                headers=headers,\n",
    "                data=payload,\n",
    "                files=files,\n",
    "                timeout=timeout\n",
    "            )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            result[\"error\"] = f\"OCR failed: HTTP {response.status_code}\"\n",
    "            return result\n",
    "        \n",
    "        # Obtener URL del PDF procesado\n",
    "        data = response.json()\n",
    "        output_url = data.get('outputUrl')\n",
    "        \n",
    "        if not output_url:\n",
    "            result[\"error\"] = \"No output URL from OCR\"\n",
    "            return result\n",
    "        \n",
    "        # Paso 2: Descargar PDF procesado\n",
    "        pdf_response = requests.get(output_url, timeout=60)\n",
    "        if pdf_response.status_code != 200:\n",
    "            result[\"error\"] = f\"Download failed: HTTP {pdf_response.status_code}\"\n",
    "            return result\n",
    "        \n",
    "        # Paso 3: Extraer texto del PDF procesado\n",
    "        temp_pdf = TEMP_DIR / f\"temp_ocr_{chunk_path.name}\"\n",
    "        try:\n",
    "            with open(temp_pdf, 'wb') as f:\n",
    "                f.write(pdf_response.content)\n",
    "            \n",
    "            # Extraer texto\n",
    "            extract_url = \"https://api.pdfrest.com/extracted-text\"\n",
    "            \n",
    "            with open(temp_pdf, 'rb') as file:\n",
    "                files = [('file', (temp_pdf.name, file, 'application/pdf'))]\n",
    "                headers = {'Api-Key': api_key}\n",
    "                \n",
    "                response = requests.post(extract_url, headers=headers, files=files, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                text = response.json().get('fullText', '')\n",
    "                # Limpiar watermarks\n",
    "                text = re.sub(r'\\[pdfRest Free Demo\\]', '', text)\n",
    "                \n",
    "                result[\"success\"] = True\n",
    "                result[\"text\"] = text\n",
    "                result[\"characters\"] = len(text)\n",
    "            else:\n",
    "                result[\"error\"] = f\"Text extraction failed: HTTP {response.status_code}\"\n",
    "                \n",
    "        finally:\n",
    "            if temp_pdf.exists():\n",
    "                temp_pdf.unlink()\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        result[\"error\"] = f\"Timeout despu√©s de {timeout}s\"\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = str(e)\n",
    "    \n",
    "    result[\"processing_time\"] = time.time() - start_time\n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ apply_ocr_to_chunk() definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8978a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de extracci√≥n de patrones cargada\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 4: FUNCIONES DE EXTRACCI√ìN DE PATRONES\n",
    "# ============================================================================\n",
    "\n",
    "def extract_patterns_from_text(text: str) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Extrae patrones espec√≠ficos del texto usando regex.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        # C√≥digos MOP est√°ndar\n",
    "        'mop_codes': list(set(re.findall(r'7\\.\\d{3}\\.\\d+[a-z]*\\d*', text, re.IGNORECASE))),\n",
    "        \n",
    "        # C√≥digos ETE\n",
    "        'ete_codes': list(set(re.findall(r'ETE[\\.\\-\\s]?\\d+', text, re.IGNORECASE))),\n",
    "        \n",
    "        # Otros c√≥digos\n",
    "        'other_codes': list(set(re.findall(r'804[\\-\\.]?\\d+', text, re.IGNORECASE))),\n",
    "        \n",
    "        # C√≥digos SAFI\n",
    "        'safi_codes': list(set(re.findall(r'SAFI\\s*[:\\s]*(\\d+)', text, re.IGNORECASE))),\n",
    "        \n",
    "        # Montos en pesos chilenos\n",
    "        'montos': re.findall(r'\\$\\s*[\\d\\.,]+', text),\n",
    "        \n",
    "        # Cantidades con unidades\n",
    "        'cantidades': re.findall(r'(\\d+[\\.,]?\\d*)\\s*(km|m3|m2|m|ton|kg|gl|un|lt|h√°)', text, re.IGNORECASE),\n",
    "        \n",
    "        # Porcentajes\n",
    "        'porcentajes': re.findall(r'\\d+[\\.,]?\\d*\\s*%', text)\n",
    "    }\n",
    "    \n",
    "    # Ordenar c√≥digos\n",
    "    for key in ['mop_codes', 'ete_codes', 'other_codes', 'safi_codes']:\n",
    "        patterns[key].sort()\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de extracci√≥n de patrones cargada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 5: AN√ÅLISIS CON CLAUDE HAIKU\n",
    "# ============================================================================\n",
    "\n",
    "def advanced_haiku_analysis(text: str, filename: str, max_chars: int = 50000) -> Dict:\n",
    "    \"\"\"\n",
    "    An√°lisis detallado usando Claude 3.5 Haiku.\n",
    "    \"\"\"\n",
    "    print(f\"ü§ñ Analizando con Claude 3.5 Haiku: {filename}\")\n",
    "    \n",
    "    # Limitar texto para an√°lisis\n",
    "    text_to_analyze = text[:max_chars] if len(text) > max_chars else text\n",
    "    \n",
    "    prompt = f\"\"\"Analiza este documento t√©cnico MOP chileno y extrae informaci√≥n estructurada.\n",
    "\n",
    "DOCUMENTO: {filename}\n",
    "TEXTO (primeros {len(text_to_analyze)} caracteres):\n",
    "{text_to_analyze}\n",
    "\n",
    "Extrae la siguiente informaci√≥n en formato JSON:\n",
    "{{\n",
    "  \"proyecto\": {{\n",
    "    \"nombre\": \"nombre completo del proyecto\",\n",
    "    \"codigo_safi\": \"c√≥digo SAFI si existe\",\n",
    "    \"tipo_obra\": \"tipo de obra\",\n",
    "    \"ubicacion\": {{\n",
    "      \"region\": \"regi√≥n\",\n",
    "      \"provincia\": \"provincia\",\n",
    "      \"comunas\": [\"lista de comunas\"],\n",
    "      \"sectores\": [\"sectores espec√≠ficos\"]\n",
    "    }}\n",
    "  }},\n",
    "  \"presupuesto\": {{\n",
    "    \"total_estimado\": \"monto total si se menciona\",\n",
    "    \"items_principales\": [\n",
    "      {{\n",
    "        \"codigo\": \"c√≥digo MOP o ETE\",\n",
    "        \"descripcion\": \"descripci√≥n del item\",\n",
    "        \"unidad\": \"unidad de medida\",\n",
    "        \"cantidad\": \"cantidad\",\n",
    "        \"precio_unitario\": \"precio unitario\",\n",
    "        \"total\": \"total del item\"\n",
    "      }}\n",
    "    ]\n",
    "  }},\n",
    "  \"codigos\": {{\n",
    "    \"mop\": [\"lista de c√≥digos MOP encontrados\"],\n",
    "    \"ete\": [\"lista de c√≥digos ETE\"],\n",
    "    \"otros\": [\"otros c√≥digos\"]\n",
    "  }},\n",
    "  \"especificaciones\": {{\n",
    "    \"materiales\": [\"principales materiales\"],\n",
    "    \"equipos\": [\"equipos mencionados\"],\n",
    "    \"normativas\": [\"normas t√©cnicas aplicables\"]\n",
    "  }},\n",
    "  \"plazos\": {{\n",
    "    \"total_dias\": \"plazo total en d√≠as\",\n",
    "    \"inicio\": \"fecha de inicio si se menciona\",\n",
    "    \"termino\": \"fecha de t√©rmino si se menciona\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Responde SOLO con el JSON, sin texto adicional.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-haiku-20241022\",\n",
    "            max_tokens=4000,\n",
    "            temperature=0,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        response_text = response.content[0].text\n",
    "        \n",
    "        # Intentar parsear el JSON\n",
    "        try:\n",
    "            json_start = response_text.find('{')\n",
    "            json_end = response_text.rfind('}') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_text = response_text[json_start:json_end]\n",
    "                analysis = json.loads(json_text)\n",
    "            else:\n",
    "                analysis = {\"raw_response\": response_text}\n",
    "        except json.JSONDecodeError:\n",
    "            analysis = {\"raw_response\": response_text}\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"analysis\": analysis,\n",
    "            \"model_used\": \"claude-3.5-haiku-20241022\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error en an√°lisis: {e}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de an√°lisis con Claude cargada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 6: FUNCI√ìN DE PROCESAMIENTO OCR COMPLETO\n",
    "# ============================================================================\n",
    "\n",
    "def process_pdf_ocr(pdf_path: Path, use_parallel: bool = True, max_workers: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Procesa un PDF completo con OCR: divisi√≥n, OCR paralelo, guardado de texto.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìÑ PROCESAMIENTO OCR: {pdf_path.name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Verificar que existe\n",
    "    if not pdf_path.exists():\n",
    "        return {\"success\": False, \"error\": f\"Archivo no encontrado: {pdf_path}\"}\n",
    "    \n",
    "    # Verificar si ya existe el texto extra√≠do\n",
    "    text_file = RESULTS_DIR / f\"{pdf_path.stem}_texto.txt\"\n",
    "    if text_file.exists():\n",
    "        print(f\"‚úÖ Texto ya extra√≠do previamente: {text_file.name}\")\n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"text\": text,\n",
    "            \"cached\": True,\n",
    "            \"text_file\": str(text_file),\n",
    "            \"total_characters\": len(text)\n",
    "        }\n",
    "    \n",
    "    # Paso 1: Dividir PDF\n",
    "    chunks_dir = TEMP_DIR / f\"{pdf_path.stem}_chunks\"\n",
    "    chunks_info = splitter.split_pdf(pdf_path, chunks_dir)\n",
    "    \n",
    "    # Paso 2: Procesar chunks (paralelo o secuencial)\n",
    "    if use_parallel:\n",
    "        ocr_result = process_chunks_parallel(chunks_info, PDF_REST_API_KEY, max_workers)\n",
    "    else:\n",
    "        # Procesamiento secuencial (fallback)\n",
    "        all_texts = []\n",
    "        for chunk_path, start, end in chunks_info:\n",
    "            print(f\"   üìã Procesando p√°ginas {start}-{end}...\")\n",
    "            result = apply_ocr_to_chunk(chunk_path, PDF_REST_API_KEY)\n",
    "            if result[\"success\"]:\n",
    "                all_texts.append(result[\"text\"])\n",
    "                print(f\"      ‚úÖ {result['characters']:,} caracteres\")\n",
    "        \n",
    "        ocr_result = {\n",
    "            \"success\": len(all_texts) > 0,\n",
    "            \"text\": \"\\n\".join(all_texts),\n",
    "            \"chunks_processed\": len(chunks_info),\n",
    "            \"chunks_successful\": len(all_texts)\n",
    "        }\n",
    "    \n",
    "    if not ocr_result[\"success\"]:\n",
    "        splitter.cleanup_chunks(chunks_dir)\n",
    "        return {\"success\": False, \"error\": \"Fall√≥ la extracci√≥n OCR\"}\n",
    "    \n",
    "    text = ocr_result[\"text\"]\n",
    "    \n",
    "    # Paso 3: Guardar texto extra√≠do\n",
    "    with open(text_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    print(f\"   üíæ Texto guardado: {text_file.name}\")\n",
    "    \n",
    "    # Paso 4: Limpiar chunks temporales\n",
    "    splitter.cleanup_chunks(chunks_dir)\n",
    "    \n",
    "    # Mostrar resumen\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ OCR COMPLETADO\")\n",
    "    print(f\"   ‚è±Ô∏è Tiempo total: {total_time:.1f}s\")\n",
    "    print(f\"   üìù Caracteres extra√≠dos: {len(text):,}\")\n",
    "    print(f\"   üíæ Archivo: {text_file}\")\n",
    "    \n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"text\": text,\n",
    "        \"cached\": False,\n",
    "        \"text_file\": str(text_file),\n",
    "        \"total_characters\": len(text),\n",
    "        \"processing_time\": total_time,\n",
    "        \"chunks_processed\": ocr_result[\"chunks_processed\"],\n",
    "        \"chunks_successful\": ocr_result[\"chunks_successful\"]\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de procesamiento OCR cargada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f499ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 6-B: CORREGIR PROBLEMA DE CODIFICACI√ìN EN GUARDADO\n",
    "# ============================================================================\n",
    "\n",
    "def save_text_safe(text: str, filepath: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Guarda texto con manejo seguro de caracteres problem√°ticos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # M√©todo 1: Intentar guardar normalmente\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        return True\n",
    "    except UnicodeEncodeError:\n",
    "        try:\n",
    "            # M√©todo 2: Limpiar caracteres problem√°ticos\n",
    "            cleaned_text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(cleaned_text)\n",
    "            print(f\"   ‚ö†Ô∏è Se limpiaron caracteres problem√°ticos del texto\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error guardando texto: {e}\")\n",
    "            return False\n",
    "\n",
    "print(\"‚úÖ save_text_safe() definida para manejar problemas de codificaci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 6-C: VERSI√ìN CORREGIDA DE process_pdf_ocr\n",
    "# ============================================================================\n",
    "\n",
    "def process_pdf_ocr(pdf_path: Path, use_parallel: bool = True, max_workers: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Procesa un PDF completo con OCR - VERSI√ìN CORREGIDA.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìÑ PROCESAMIENTO OCR: {pdf_path.name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Verificar que existe\n",
    "    if not pdf_path.exists():\n",
    "        return {\"success\": False, \"error\": f\"Archivo no encontrado: {pdf_path}\"}\n",
    "    \n",
    "    # Verificar si ya existe el texto extra√≠do\n",
    "    text_file = RESULTS_DIR / f\"{pdf_path.stem}_texto.txt\"\n",
    "    if text_file.exists():\n",
    "        print(f\"‚úÖ Texto ya extra√≠do previamente: {text_file.name}\")\n",
    "        try:\n",
    "            with open(text_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"text\": text,\n",
    "                \"cached\": True,\n",
    "                \"text_file\": str(text_file),\n",
    "                \"total_characters\": len(text)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error leyendo archivo existente: {e}\")\n",
    "    \n",
    "    # Paso 1: Dividir PDF\n",
    "    chunks_dir = TEMP_DIR / f\"{pdf_path.stem}_chunks\"\n",
    "    chunks_info = splitter.split_pdf(pdf_path, chunks_dir)\n",
    "    \n",
    "    if not chunks_info:\n",
    "        return {\"success\": False, \"error\": \"No se pudieron crear chunks del PDF\"}\n",
    "    \n",
    "    # Paso 2: Procesar chunks\n",
    "    if use_parallel:\n",
    "        ocr_result = process_chunks_parallel(chunks_info, PDF_REST_API_KEY, max_workers)\n",
    "    else:\n",
    "        # Procesamiento secuencial\n",
    "        all_texts = []\n",
    "        for chunk_path, start, end in chunks_info:\n",
    "            print(f\"   üìã Procesando p√°ginas {start}-{end}...\")\n",
    "            result = apply_ocr_to_chunk(chunk_path, PDF_REST_API_KEY)\n",
    "            if result[\"success\"]:\n",
    "                all_texts.append(result[\"text\"])\n",
    "                print(f\"      ‚úÖ {result['characters']:,} caracteres\")\n",
    "        \n",
    "        ocr_result = {\n",
    "            \"success\": len(all_texts) > 0,\n",
    "            \"text\": \"\\n\".join(all_texts),\n",
    "            \"chunks_processed\": len(chunks_info),\n",
    "            \"chunks_successful\": len(all_texts)\n",
    "        }\n",
    "    \n",
    "    if not ocr_result[\"success\"]:\n",
    "        splitter.cleanup_chunks(chunks_dir)\n",
    "        return {\"success\": False, \"error\": \"Fall√≥ la extracci√≥n OCR\"}\n",
    "    \n",
    "    text = ocr_result[\"text\"]\n",
    "    \n",
    "    # Paso 3: Guardar texto extra√≠do con manejo seguro\n",
    "    if save_text_safe(text, text_file):\n",
    "        print(f\"   üíæ Texto guardado: {text_file.name}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è No se pudo guardar el texto, pero continuando...\")\n",
    "    \n",
    "    # Paso 4: Limpiar chunks temporales\n",
    "    splitter.cleanup_chunks(chunks_dir)\n",
    "    \n",
    "    # Mostrar resumen\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ OCR COMPLETADO\")\n",
    "    print(f\"   ‚è±Ô∏è Tiempo total: {total_time:.1f}s\")\n",
    "    print(f\"   üìù Caracteres extra√≠dos: {len(text):,}\")\n",
    "    print(f\"   üíæ Archivo: {text_file}\")\n",
    "    \n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"text\": text,\n",
    "        \"cached\": False,\n",
    "        \"text_file\": str(text_file),\n",
    "        \"total_characters\": len(text),\n",
    "        \"processing_time\": total_time,\n",
    "        \"chunks_processed\": ocr_result[\"chunks_processed\"],\n",
    "        \"chunks_successful\": ocr_result[\"chunks_successful\"]\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ process_pdf_ocr() corregida con manejo seguro de codificaci√≥n\")\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICACI√ìN FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TODAS LAS FUNCIONES NECESARIAS HAN SIDO DEFINIDAS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFunciones disponibles:\")\n",
    "print(\"  ‚Ä¢ apply_ocr_to_chunk() - OCR individual\")\n",
    "print(\"  ‚Ä¢ process_chunks_parallel() - Procesamiento paralelo\")\n",
    "print(\"  ‚Ä¢ process_pdf_ocr() - Procesamiento completo con manejo de errores\")\n",
    "print(\"  ‚Ä¢ save_text_safe() - Guardado seguro de texto\")\n",
    "print(\"  ‚Ä¢ CONFIG - Configuraci√≥n global del sistema\")\n",
    "print(\"\\nAhora puedes ejecutar:\")\n",
    "print(\"  >>> results = process_all_bases_complete()\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e05679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 7: FUNCI√ìN DE AN√ÅLISIS COMPLETO\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_pdf_document(pdf_path: Path, force_ocr: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    An√°lisis completo de un documento PDF: OCR + Extracci√≥n + An√°lisis IA.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìö AN√ÅLISIS COMPLETO: {pdf_path.name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if not pdf_path.exists():\n",
    "        print(f\"‚ùå Archivo no encontrado: {pdf_path}\")\n",
    "        return {\"success\": False, \"error\": \"Archivo no encontrado\"}\n",
    "    \n",
    "    # PASO 1: OCR - Extraer texto\n",
    "    print(f\"\\nüìù PASO 1: Extracci√≥n de texto...\")\n",
    "    ocr_result = process_pdf_ocr(pdf_path, use_parallel=True, max_workers=3)\n",
    "    \n",
    "    if not ocr_result[\"success\"]:\n",
    "        return ocr_result\n",
    "    \n",
    "    text = ocr_result[\"text\"]\n",
    "    \n",
    "    # Verificar si hay texto extra√≠do\n",
    "    if not text or len(text) < 100:\n",
    "        print(f\"‚ö†Ô∏è Texto extra√≠do muy corto ({len(text)} caracteres)\")\n",
    "        print(\"   Posible problema con el OCR o PDF sin texto\")\n",
    "    \n",
    "    # PASO 2: Extracci√≥n de patrones\n",
    "    print(f\"\\nüìä PASO 2: Extrayendo patrones...\")\n",
    "    patterns = extract_patterns_from_text(text)\n",
    "    \n",
    "    print(f\"   ‚úÖ C√≥digos MOP: {len(patterns['mop_codes'])}\")\n",
    "    print(f\"   ‚úÖ C√≥digos ETE: {len(patterns['ete_codes'])}\")\n",
    "    print(f\"   ‚úÖ C√≥digos SAFI: {len(patterns['safi_codes'])}\")\n",
    "    print(f\"   ‚úÖ Montos encontrados: {len(patterns['montos'])}\")\n",
    "    \n",
    "    # PASO 3: An√°lisis con Claude\n",
    "    print(f\"\\nü§ñ PASO 3: An√°lisis con IA...\")\n",
    "    ai_analysis = advanced_haiku_analysis(text, pdf_path.name)\n",
    "    \n",
    "    if ai_analysis['success']:\n",
    "        print(f\"   ‚úÖ An√°lisis completado\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Error en an√°lisis: {ai_analysis.get('error')}\")\n",
    "    \n",
    "    # PASO 4: Consolidar resultados\n",
    "    final_result = {\n",
    "        \"success\": True,\n",
    "        \"filename\": pdf_path.name,\n",
    "        \"file_path\": str(pdf_path),\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"extraction\": {\n",
    "            \"text_file\": ocr_result[\"text_file\"],\n",
    "            \"total_characters\": len(text),\n",
    "            \"total_lines\": len(text.split('\\n')),\n",
    "            \"total_words\": len(text.split()),\n",
    "            \"cached\": ocr_result.get(\"cached\", False)\n",
    "        },\n",
    "        \"patterns_extracted\": patterns,\n",
    "        \"ai_analysis\": ai_analysis,\n",
    "        \"summary\": {}\n",
    "    }\n",
    "    \n",
    "    # Crear resumen ejecutivo\n",
    "    if ai_analysis.get('success') and isinstance(ai_analysis.get('analysis'), dict):\n",
    "        analysis = ai_analysis['analysis']\n",
    "        \n",
    "        # Acceso seguro a estructuras anidadas\n",
    "        proyecto = analysis.get('proyecto', {}) or {}\n",
    "        presupuesto = analysis.get('presupuesto', {}) or {}\n",
    "        ubicacion = proyecto.get('ubicacion', {}) or {}\n",
    "        \n",
    "        final_result['summary'] = {\n",
    "            'proyecto': proyecto.get('nombre', 'No identificado') or 'No identificado',\n",
    "            'region': ubicacion.get('region', 'N/D') or 'N/D',\n",
    "            'comunas': ubicacion.get('comunas', []) or [],\n",
    "            'tipo_obra': proyecto.get('tipo_obra', 'N/D') or 'N/D',\n",
    "            'presupuesto_estimado': presupuesto.get('total_estimado', 'N/D') or 'N/D',\n",
    "            'items_principales': len(presupuesto.get('items_principales', []) or []),\n",
    "            'codigos_mop': len(patterns['mop_codes']),\n",
    "            'codigos_ete': len(patterns['ete_codes']),\n",
    "            'codigos_safi': len(patterns['safi_codes'])\n",
    "        }\n",
    "    else:\n",
    "        # Si no hay an√°lisis o fall√≥, crear resumen b√°sico\n",
    "        final_result['summary'] = {\n",
    "            'proyecto': 'No identificado',\n",
    "            'region': 'N/D',\n",
    "            'comunas': [],\n",
    "            'tipo_obra': 'N/D',\n",
    "            'presupuesto_estimado': 'N/D',\n",
    "            'items_principales': 0,\n",
    "            'codigos_mop': len(patterns['mop_codes']),\n",
    "            'codigos_ete': len(patterns['ete_codes']),\n",
    "            'codigos_safi': len(patterns['safi_codes'])\n",
    "        }\n",
    "    \n",
    "    # PASO 5: Guardar resultados\n",
    "    analysis_file = RESULTS_DIR / f\"{pdf_path.stem}_analisis_completo.json\"\n",
    "    with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_result, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ AN√ÅLISIS COMPLETADO\")\n",
    "    print(f\"   üíæ Resultados guardados: {analysis_file.name}\")\n",
    "    \n",
    "    # Mostrar resumen\n",
    "    s = final_result['summary']\n",
    "    print(f\"\\nüìã RESUMEN:\")\n",
    "    print(f\"   Proyecto: {s['proyecto'][:60]}\")\n",
    "    print(f\"   Ubicaci√≥n: {s['region']}, {', '.join(s['comunas'][:3]) if s['comunas'] else 'N/D'}\")\n",
    "    print(f\"   Tipo obra: {s['tipo_obra']}\")\n",
    "    print(f\"   Presupuesto: {s['presupuesto_estimado']}\")\n",
    "    print(f\"   Items: {s['items_principales']}\")\n",
    "    print(f\"   C√≥digos: {s['codigos_mop']} MOP, {s['codigos_ete']} ETE, {s['codigos_safi']} SAFI\")\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de an√°lisis completo cargada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 8: PROCESAR TODOS LOS ARCHIVOS - CORREGIDO\n",
    "# ============================================================================\n",
    "\n",
    "def process_all_bases_complete():\n",
    "    \"\"\"\n",
    "    Procesa TODOS los PDFs base con an√°lisis completo.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ PROCESAMIENTO COMPLETO DE DOCUMENTOS MOP\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Lista de TODOS los PDFs a procesar\n",
    "    pdf_files = []\n",
    "    \n",
    "    # Buscar todos los PDFs en el directorio bases\n",
    "    for pdf_file in BASES_DIR.glob(\"*.pdf\"):\n",
    "        pdf_files.append(pdf_file)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(\"‚ùå No se encontraron archivos PDF en el directorio bases\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nüìö Archivos encontrados: {len(pdf_files)}\")\n",
    "    for pdf in pdf_files:\n",
    "        print(f\"   - {pdf.name} ({pdf.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
    "    \n",
    "    # Procesar cada archivo\n",
    "    all_results = []\n",
    "    summary_data = []\n",
    "    \n",
    "    for idx, pdf_path in enumerate(pdf_files, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìÑ [{idx}/{len(pdf_files)}] Procesando: {pdf_path.name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Analizar el documento\n",
    "            result = analyze_pdf_document(pdf_path, force_ocr=False)\n",
    "            all_results.append(result)\n",
    "            \n",
    "            # Agregar al resumen\n",
    "            if result.get('success'):\n",
    "                summary_data.append({\n",
    "                    'Archivo': pdf_path.name,\n",
    "                    'Tama√±o (MB)': round(pdf_path.stat().st_size / 1024 / 1024, 2),\n",
    "                    'Caracteres': result['extraction']['total_characters'],\n",
    "                    'Palabras': result['extraction']['total_words'],\n",
    "                    'C√≥digos MOP': result['summary'].get('codigos_mop', 0),\n",
    "                    'C√≥digos ETE': result['summary'].get('codigos_ete', 0),\n",
    "                    'C√≥digos SAFI': result['summary'].get('codigos_safi', 0),\n",
    "                    'Items': result['summary'].get('items_principales', 0),\n",
    "                    'Regi√≥n': result['summary'].get('region', 'N/D'),\n",
    "                    'Tipo Obra': result['summary'].get('tipo_obra', 'N/D')[:30],\n",
    "                    'Estado': '‚úÖ Completado'\n",
    "                })\n",
    "            else:\n",
    "                summary_data.append({\n",
    "                    'Archivo': pdf_path.name,\n",
    "                    'Tama√±o (MB)': round(pdf_path.stat().st_size / 1024 / 1024, 2),\n",
    "                    'Estado': f'‚ùå Error: {result.get(\"error\", \"Desconocido\")}'\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error procesando {pdf_path.name}: {e}\")\n",
    "            summary_data.append({\n",
    "                'Archivo': pdf_path.name,\n",
    "                'Estado': f'‚ùå Excepci√≥n: {str(e)[:50]}'\n",
    "            })\n",
    "    \n",
    "    # Mostrar resumen consolidado\n",
    "    if summary_data:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"üìä RESUMEN CONSOLIDADO\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df_summary = pd.DataFrame(summary_data)\n",
    "        display(df_summary)\n",
    "        \n",
    "        # Estad√≠sticas globales\n",
    "        successful = len([s for s in summary_data if '‚úÖ' in s.get('Estado', '')])\n",
    "        failed = len(summary_data) - successful\n",
    "        \n",
    "        print(f\"\\nüìà ESTAD√çSTICAS TOTALES:\")\n",
    "        print(f\"   üìÑ Documentos procesados: {successful}/{len(pdf_files)}\")\n",
    "        print(f\"   ‚ùå Documentos con errores: {failed}\")\n",
    "        \n",
    "        if 'Caracteres' in df_summary.columns:\n",
    "            total_chars = df_summary['Caracteres'].fillna(0).sum()\n",
    "            avg_chars = df_summary['Caracteres'].fillna(0).mean()\n",
    "            print(f\"   üìù Total caracteres: {int(total_chars):,}\")\n",
    "            print(f\"   üìä Promedio caracteres: {int(avg_chars):,}\")\n",
    "        \n",
    "        if 'C√≥digos MOP' in df_summary.columns:\n",
    "            total_mop = df_summary['C√≥digos MOP'].fillna(0).sum()\n",
    "            print(f\"   üî¢ Total c√≥digos MOP: {int(total_mop)}\")\n",
    "        \n",
    "        # Guardar resumen en Excel\n",
    "        excel_path = RESULTS_DIR / f\"resumen_consolidado_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "        df_summary.to_excel(excel_path, index=False)\n",
    "        print(f\"\\nüíæ Resumen guardado en: {excel_path}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"‚úÖ Funci√≥n process_all_bases_complete() cargada\")\n",
    "print(\"   Esta funci√≥n S√ç procesa TODOS los archivos PDF del directorio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cc90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 9: EJECUTAR PROCESAMIENTO (EJECUTAR ESTA CELDA PARA INICIAR)\n",
    "# ============================================================================\n",
    "\n",
    "results = process_all_bases_complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea253221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 10: VERIFICAR ARCHIVOS EXISTENTES\n",
    "# ============================================================================\n",
    "\n",
    "def check_existing_files():\n",
    "    \"\"\"\n",
    "    Verifica qu√© archivos ya han sido procesados y cu√°les faltan.\n",
    "    \"\"\"\n",
    "    print(\"üìÅ VERIFICACI√ìN DE ARCHIVOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Archivos PDF base\n",
    "    pdf_files = [\"bases1.pdf\", \"bases2.pdf\", \"bases3.pdf\"]\n",
    "    \n",
    "    for pdf_name in pdf_files:\n",
    "        pdf_path = BASES_DIR / pdf_name\n",
    "        text_file = RESULTS_DIR / f\"{pdf_path.stem}_texto.txt\"\n",
    "        analysis_file = RESULTS_DIR / f\"{pdf_path.stem}_analisis_completo.json\"\n",
    "        \n",
    "        print(f\"\\nüìÑ {pdf_name}:\")\n",
    "        \n",
    "        # Verificar PDF original\n",
    "        if pdf_path.exists():\n",
    "            size_mb = pdf_path.stat().st_size / 1024 / 1024\n",
    "            print(f\"   ‚úÖ PDF existe ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå PDF NO encontrado\")\n",
    "            continue\n",
    "        \n",
    "        # Verificar texto extra√≠do\n",
    "        if text_file.exists():\n",
    "            with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                text_len = len(f.read())\n",
    "            print(f\"   ‚úÖ Texto extra√≠do ({text_len:,} caracteres)\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Texto NO extra√≠do - requiere OCR\")\n",
    "        \n",
    "        # Verificar an√°lisis\n",
    "        if analysis_file.exists():\n",
    "            print(f\"   ‚úÖ An√°lisis completo existe\")\n",
    "            with open(analysis_file, 'r', encoding='utf-8') as f:\n",
    "                analysis = json.load(f)\n",
    "                if 'summary' in analysis:\n",
    "                    s = analysis['summary']\n",
    "                    print(f\"      - Proyecto: {s.get('proyecto', 'N/D')[:50]}\")\n",
    "                    print(f\"      - C√≥digos MOP: {s.get('codigos_mop', 0)}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è An√°lisis NO realizado\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    return True\n",
    "\n",
    "# Ejecutar verificaci√≥n\n",
    "check_existing_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461151a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 11: REPROCESAR ARCHIVO ESPEC√çFICO - MEJORADO\n",
    "# ============================================================================\n",
    "\n",
    "def reprocess_file_enhanced(filename: str, force_ocr: bool = True, \n",
    "                           max_workers: int = 4, chunk_size: int = 15):\n",
    "    \"\"\"\n",
    "    Reprocesa un archivo con configuraci√≥n personalizada.\n",
    "    \n",
    "    Args:\n",
    "        filename: Nombre del archivo o path completo\n",
    "        force_ocr: Forzar nuevo OCR\n",
    "        max_workers: N√∫mero de workers paralelos\n",
    "        chunk_size: P√°ginas por chunk\n",
    "    \"\"\"\n",
    "    # Buscar archivo\n",
    "    if Path(filename).exists():\n",
    "        pdf_path = Path(filename)\n",
    "    else:\n",
    "        pdf_path = BASES_DIR / filename\n",
    "    \n",
    "    if not pdf_path.exists():\n",
    "        print(f\"‚ùå Archivo no encontrado: {filename}\")\n",
    "        print(f\"   Buscado en: {pdf_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüîÑ REPROCESAMIENTO AVANZADO: {pdf_path.name}\")\n",
    "    print(f\"   ‚öôÔ∏è Configuraci√≥n:\")\n",
    "    print(f\"      - Workers: {max_workers}\")\n",
    "    print(f\"      - Chunk size: {chunk_size} p√°ginas\")\n",
    "    print(f\"      - Forzar OCR: {force_ocr}\")\n",
    "    \n",
    "    # Ajustar configuraci√≥n temporal\n",
    "    old_workers = CONFIG['MAX_WORKERS']\n",
    "    old_chunk = CONFIG['CHUNK_SIZE']\n",
    "    \n",
    "    CONFIG['MAX_WORKERS'] = max_workers\n",
    "    CONFIG['CHUNK_SIZE'] = chunk_size\n",
    "    \n",
    "    if force_ocr:\n",
    "        # Eliminar archivos existentes\n",
    "        text_file = RESULTS_DIR / f\"{pdf_path.stem}_texto.txt\"\n",
    "        if text_file.exists():\n",
    "            text_file.unlink()\n",
    "            print(f\"   üóëÔ∏è Archivo de texto anterior eliminado\")\n",
    "    \n",
    "    # Procesar\n",
    "    result = analyze_pdf_document(pdf_path, force_ocr=force_ocr)\n",
    "    \n",
    "    # Restaurar configuraci√≥n\n",
    "    CONFIG['MAX_WORKERS'] = old_workers\n",
    "    CONFIG['CHUNK_SIZE'] = old_chunk\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\n‚úÖ Reprocesamiento exitoso:\")\n",
    "        print(f\"   üìù Caracteres extra√≠dos: {result['extraction']['total_characters']:,}\")\n",
    "        print(f\"   üî¢ C√≥digos MOP: {result['summary']['codigos_mop']}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Reprocesamiento fallido: {result.get('error')}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Funci√≥n reprocess_file_enhanced() cargada\")\n",
    "print(\"   Ejemplo: reprocess_file_enhanced('bases3.pdf', max_workers=6, chunk_size=10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 12: GENERAR INFORME HTML CONSOLIDADO - COMPLETO\n",
    "# ============================================================================\n",
    "\n",
    "def generate_consolidated_report():\n",
    "    \"\"\"\n",
    "    Genera un informe HTML consolidado de todos los an√°lisis.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä GENERANDO INFORME CONSOLIDADO HTML\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Recopilar todos los an√°lisis\n",
    "    analysis_files = list(RESULTS_DIR.glob(\"*_analisis_completo.json\"))\n",
    "    \n",
    "    if not analysis_files:\n",
    "        print(\"‚ö†Ô∏è No se encontraron an√°lisis para consolidar\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÑ Archivos de an√°lisis encontrados: {len(analysis_files)}\")\n",
    "    \n",
    "    all_data = []\n",
    "    for file in analysis_files:\n",
    "        print(f\"   - Cargando: {file.name}\")\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            all_data.append(data)\n",
    "    \n",
    "    # Estad√≠sticas globales\n",
    "    total_chars = sum(data['extraction']['total_characters'] for data in all_data)\n",
    "    total_mop = sum(data['summary'].get('codigos_mop', 0) for data in all_data)\n",
    "    total_items = sum(data['summary'].get('items_principales', 0) for data in all_data)\n",
    "    total_pages = sum(data['extraction'].get('total_pages', 0) for data in all_data)\n",
    "    \n",
    "    # Crear HTML completo\n",
    "    html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Informe Consolidado MOP - An√°lisis de Presupuestos</title>\n",
    "    <style>\n",
    "        * {{\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            box-sizing: border-box;\n",
    "        }}\n",
    "        \n",
    "        body {{\n",
    "            font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, Arial, sans-serif;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            min-height: 100vh;\n",
    "            padding: 20px;\n",
    "            line-height: 1.6;\n",
    "        }}\n",
    "        \n",
    "        .container {{\n",
    "            max-width: 1400px;\n",
    "            margin: 0 auto;\n",
    "            background: white;\n",
    "            border-radius: 20px;\n",
    "            box-shadow: 0 20px 60px rgba(0,0,0,0.3);\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        \n",
    "        .header {{\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 40px;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        \n",
    "        .header h1 {{\n",
    "            font-size: 2.8em;\n",
    "            margin-bottom: 15px;\n",
    "            font-weight: 300;\n",
    "            text-shadow: 0 2px 4px rgba(0,0,0,0.3);\n",
    "        }}\n",
    "        \n",
    "        .header .subtitle {{\n",
    "            font-size: 1.3em;\n",
    "            opacity: 0.9;\n",
    "            margin-bottom: 20px;\n",
    "        }}\n",
    "        \n",
    "        .header .date {{\n",
    "            font-size: 1em;\n",
    "            opacity: 0.8;\n",
    "            background: rgba(255,255,255,0.1);\n",
    "            padding: 10px 20px;\n",
    "            border-radius: 25px;\n",
    "            display: inline-block;\n",
    "        }}\n",
    "        \n",
    "        .content {{\n",
    "            padding: 40px;\n",
    "        }}\n",
    "        \n",
    "        .summary-section {{\n",
    "            margin-bottom: 50px;\n",
    "        }}\n",
    "        \n",
    "        .section-title {{\n",
    "            font-size: 2em;\n",
    "            color: #333;\n",
    "            margin-bottom: 30px;\n",
    "            text-align: center;\n",
    "            position: relative;\n",
    "        }}\n",
    "        \n",
    "        .section-title::after {{\n",
    "            content: '';\n",
    "            position: absolute;\n",
    "            bottom: -10px;\n",
    "            left: 50%;\n",
    "            transform: translateX(-50%);\n",
    "            width: 60px;\n",
    "            height: 3px;\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "            border-radius: 2px;\n",
    "        }}\n",
    "        \n",
    "        .summary-grid {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));\n",
    "            gap: 25px;\n",
    "            margin: 40px 0;\n",
    "        }}\n",
    "        \n",
    "        .summary-card {{\n",
    "            background: white;\n",
    "            border-radius: 15px;\n",
    "            padding: 30px;\n",
    "            text-align: center;\n",
    "            box-shadow: 0 8px 25px rgba(0,0,0,0.1);\n",
    "            transition: all 0.3s ease;\n",
    "            border: 2px solid transparent;\n",
    "            position: relative;\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        \n",
    "        .summary-card::before {{\n",
    "            content: '';\n",
    "            position: absolute;\n",
    "            top: 0;\n",
    "            left: 0;\n",
    "            right: 0;\n",
    "            height: 4px;\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "        }}\n",
    "        \n",
    "        .summary-card:hover {{\n",
    "            transform: translateY(-5px);\n",
    "            box-shadow: 0 15px 35px rgba(0,0,0,0.15);\n",
    "        }}\n",
    "        \n",
    "        .summary-value {{\n",
    "            font-size: 2.5em;\n",
    "            font-weight: 700;\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "            -webkit-background-clip: text;\n",
    "            -webkit-text-fill-color: transparent;\n",
    "            background-clip: text;\n",
    "            margin-bottom: 10px;\n",
    "        }}\n",
    "        \n",
    "        .summary-label {{\n",
    "            color: #666;\n",
    "            font-size: 1.1em;\n",
    "            font-weight: 500;\n",
    "            text-transform: uppercase;\n",
    "            letter-spacing: 1px;\n",
    "        }}\n",
    "        \n",
    "        .documents-table {{\n",
    "            width: 100%;\n",
    "            background: white;\n",
    "            border-radius: 15px;\n",
    "            overflow: hidden;\n",
    "            box-shadow: 0 8px 25px rgba(0,0,0,0.1);\n",
    "            margin-top: 30px;\n",
    "        }}\n",
    "        \n",
    "        .documents-table th {{\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "            color: white;\n",
    "            padding: 20px 15px;\n",
    "            text-align: left;\n",
    "            font-weight: 600;\n",
    "            text-transform: uppercase;\n",
    "            letter-spacing: 0.5px;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        \n",
    "        .documents-table td {{\n",
    "            padding: 18px 15px;\n",
    "            border-bottom: 1px solid #eee;\n",
    "            vertical-align: top;\n",
    "        }}\n",
    "        \n",
    "        .documents-table tr:last-child td {{\n",
    "            border-bottom: none;\n",
    "        }}\n",
    "        \n",
    "        .documents-table tbody tr:hover {{\n",
    "            background: #f8f9ff;\n",
    "        }}\n",
    "        \n",
    "        .doc-name {{\n",
    "            font-weight: 600;\n",
    "            color: #333;\n",
    "            font-size: 1em;\n",
    "        }}\n",
    "        \n",
    "        .project-name {{\n",
    "            color: #555;\n",
    "            max-width: 300px;\n",
    "            word-wrap: break-word;\n",
    "        }}\n",
    "        \n",
    "        .no-data {{\n",
    "            color: #999;\n",
    "            font-style: italic;\n",
    "        }}\n",
    "        \n",
    "        .mop-count {{\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "            color: white;\n",
    "            padding: 5px 12px;\n",
    "            border-radius: 20px;\n",
    "            font-size: 0.9em;\n",
    "            font-weight: 600;\n",
    "            display: inline-block;\n",
    "        }}\n",
    "        \n",
    "        .items-count {{\n",
    "            background: #28a745;\n",
    "            color: white;\n",
    "            padding: 5px 12px;\n",
    "            border-radius: 20px;\n",
    "            font-size: 0.9em;\n",
    "            font-weight: 600;\n",
    "            display: inline-block;\n",
    "        }}\n",
    "        \n",
    "        .chars-count {{\n",
    "            color: #666;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        \n",
    "        .location {{\n",
    "            color: #555;\n",
    "            font-size: 0.95em;\n",
    "        }}\n",
    "        \n",
    "        .footer {{\n",
    "            text-align: center;\n",
    "            padding: 30px;\n",
    "            background: #f8f9fa;\n",
    "            color: #666;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        \n",
    "        @media (max-width: 768px) {{\n",
    "            .header h1 {{ font-size: 2em; }}\n",
    "            .summary-grid {{ grid-template-columns: 1fr; }}\n",
    "            .content {{ padding: 20px; }}\n",
    "            .documents-table {{ font-size: 0.9em; }}\n",
    "            .documents-table th, .documents-table td {{ padding: 12px 8px; }}\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>üìä Informe Consolidado MOP</h1>\n",
    "            <div class=\"subtitle\">An√°lisis de Bases de Licitaci√≥n y Presupuestos</div>\n",
    "            <div class=\"date\">üìÖ Generado: {datetime.now().strftime('%d de %B de %Y a las %H:%M hrs')}</div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"content\">\n",
    "            <div class=\"summary-section\">\n",
    "                <h2 class=\"section-title\">Resumen Ejecutivo</h2>\n",
    "                <div class=\"summary-grid\">\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{len(all_data)}</div>\n",
    "                        <div class=\"summary-label\">Documentos Procesados</div>\n",
    "                    </div>\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{total_pages}</div>\n",
    "                        <div class=\"summary-label\">P√°ginas Totales</div>\n",
    "                    </div>\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{total_chars:,}</div>\n",
    "                        <div class=\"summary-label\">Caracteres Extra√≠dos</div>\n",
    "                    </div>\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{total_mop}</div>\n",
    "                        <div class=\"summary-label\">C√≥digos MOP Identificados</div>\n",
    "                    </div>\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{total_items}</div>\n",
    "                        <div class=\"summary-label\">Items Presupuestarios</div>\n",
    "                    </div>\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{total_chars//1000:.0f}K</div>\n",
    "                        <div class=\"summary-label\">Promedio Caracteres</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"documents-section\">\n",
    "                <h2 class=\"section-title\">Detalle por Documento</h2>\n",
    "                <table class=\"documents-table\">\n",
    "                    <thead>\n",
    "                        <tr>\n",
    "                            <th>üìÑ Documento</th>\n",
    "                            <th>üèóÔ∏è Proyecto</th>\n",
    "                            <th>üìç Ubicaci√≥n</th>\n",
    "                            <th>üîß Tipo de Obra</th>\n",
    "                            <th>üî¢ C√≥digos MOP</th>\n",
    "                            <th>üìã Items</th>\n",
    "                            <th>üìù Contenido</th>\n",
    "                        </tr>\n",
    "                    </thead>\n",
    "                    <tbody>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Agregar filas de documentos\n",
    "    for data in sorted(all_data, key=lambda x: x['filename']):\n",
    "        s = data['summary']\n",
    "        \n",
    "        # Procesar datos\n",
    "        comunas = ', '.join(s.get('comunas', [])[:2]) if s.get('comunas') else 'N/D'\n",
    "        if len(s.get('comunas', [])) > 2:\n",
    "            comunas += f\" (+{len(s.get('comunas', [])) - 2} m√°s)\"\n",
    "        \n",
    "        proyecto = s.get('proyecto', 'No identificado')\n",
    "        if proyecto == 'No identificado':\n",
    "            proyecto = '<span class=\"no-data\">No identificado</span>'\n",
    "        else:\n",
    "            proyecto = proyecto[:80] + ('...' if len(proyecto) > 80 else '')\n",
    "        \n",
    "        region = s.get('region', 'N/D')\n",
    "        tipo_obra = s.get('tipo_obra', 'N/D')\n",
    "        if tipo_obra != 'N/D':\n",
    "            tipo_obra = tipo_obra[:40] + ('...' if len(tipo_obra) > 40 else '')\n",
    "        \n",
    "        mop_count = s.get('codigos_mop', 0)\n",
    "        items_count = s.get('items_principales', 0)\n",
    "        chars_count = data['extraction']['total_characters']\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "                        <tr>\n",
    "                            <td><div class=\"doc-name\">{data['filename']}</div></td>\n",
    "                            <td><div class=\"project-name\">{proyecto}</div></td>\n",
    "                            <td><div class=\"location\">{region}<br>{comunas}</div></td>\n",
    "                            <td>{tipo_obra}</td>\n",
    "                            <td><span class=\"mop-count\">{mop_count}</span></td>\n",
    "                            <td><span class=\"items-count\">{items_count}</span></td>\n",
    "                            <td><div class=\"chars-count\">{chars_count:,} chars</div></td>\n",
    "                        </tr>\n",
    "        \"\"\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "                    </tbody>\n",
    "                </table>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"footer\">\n",
    "            <p>ü§ñ Generado autom√°ticamente por MOP Analyzer v2.0</p>\n",
    "            <p>Sistema de an√°lisis de documentos de licitaci√≥n del Ministerio de Obras P√∫blicas</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Guardar archivo\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_file = RESULTS_DIR / f\"informe_consolidado_{timestamp}.html\"\n",
    "    \n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    # Tambi√©n crear una versi√≥n sin timestamp\n",
    "    report_file_simple = RESULTS_DIR / \"informe_consolidado.html\"\n",
    "    with open(report_file_simple, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"\\n‚úÖ INFORME HTML GENERADO EXITOSAMENTE\")\n",
    "    print(f\"üìÅ Archivo principal: {report_file}\")\n",
    "    print(f\"üìÅ Acceso directo: {report_file_simple}\")\n",
    "    print(f\"üåê Para ver el informe, abre el archivo HTML en tu navegador\")\n",
    "    \n",
    "    # Mostrar estad√≠sticas finales\n",
    "    print(f\"\\nüìä ESTAD√çSTICAS DEL INFORME:\")\n",
    "    print(f\"   üìÑ Documentos procesados: {len(all_data)}\")\n",
    "    print(f\"   üìù Total caracteres: {total_chars:,}\")\n",
    "    print(f\"   üî¢ Total c√≥digos MOP: {total_mop}\")\n",
    "    print(f\"   üìã Total items: {total_items}\")\n",
    "    \n",
    "    # Si estamos en Jupyter, mostrar el HTML\n",
    "    try:\n",
    "        from IPython.display import HTML, display\n",
    "        display(HTML(f'<p style=\"color: green; font-weight: bold;\">‚úÖ Informe generado: <a href=\"{report_file.name}\" target=\"_blank\">{report_file.name}</a></p>'))\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'report_file': report_file,\n",
    "        'report_file_simple': report_file_simple,\n",
    "        'total_documents': len(all_data),\n",
    "        'total_characters': total_chars,\n",
    "        'total_mop_codes': total_mop,\n",
    "        'total_items': total_items\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de generaci√≥n de informe HTML COMPLETA cargada\")\n",
    "print(\"\\nüöÄ USAR: generate_consolidated_report()\")\n",
    "print(\"üìÑ Generar√° un informe HTML profesional con todos los an√°lisis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f28cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 12-b\n",
    "generate_consolidated_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 13: DIAGN√ìSTICO DE PROBLEMAS CON BASES3.PDF\n",
    "# ============================================================================\n",
    "\n",
    "def diagnose_pdf_issues(filename: str):\n",
    "    \"\"\"\n",
    "    Diagnostica problemas con un PDF espec√≠fico.\n",
    "    \"\"\"\n",
    "    pdf_path = BASES_DIR / filename\n",
    "    \n",
    "    print(f\"\\nüîç DIAGN√ìSTICO: {filename}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not pdf_path.exists():\n",
    "        print(f\"‚ùå Archivo no encontrado\")\n",
    "        return\n",
    "    \n",
    "    # Informaci√≥n b√°sica\n",
    "    size_mb = pdf_path.stat().st_size / 1024 / 1024\n",
    "    print(f\"üìä Tama√±o: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # Intentar leer con PyPDF2\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(reader.pages)\n",
    "            print(f\"üìÑ P√°ginas: {num_pages}\")\n",
    "            \n",
    "            # Intentar extraer texto de las primeras p√°ginas\n",
    "            text_sample = \"\"\n",
    "            for i in range(min(3, num_pages)):\n",
    "                try:\n",
    "                    page_text = reader.pages[i].extract_text()\n",
    "                    text_sample += page_text\n",
    "                    print(f\"   P√°gina {i+1}: {len(page_text)} caracteres\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   P√°gina {i+1}: Error - {e}\")\n",
    "            \n",
    "            if text_sample:\n",
    "                print(f\"\\nüìù Muestra de texto (primeros 500 caracteres):\")\n",
    "                print(text_sample[:500])\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è No se pudo extraer texto con PyPDF2\")\n",
    "                print(\"   El PDF podr√≠a ser:\")\n",
    "                print(\"   - Un documento escaneado (requiere OCR)\")\n",
    "                print(\"   - Un PDF protegido\")\n",
    "                print(\"   - Un PDF corrupto\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error leyendo PDF: {e}\")\n",
    "    \n",
    "    # Verificar archivos existentes\n",
    "    text_file = RESULTS_DIR / f\"{pdf_path.stem}_texto.txt\"\n",
    "    if text_file.exists():\n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            existing_text = f.read()\n",
    "        print(f\"\\nüìÑ Archivo de texto existe: {len(existing_text)} caracteres\")\n",
    "        if len(existing_text) < 100:\n",
    "            print(\"   ‚ö†Ô∏è El archivo de texto est√° casi vac√≠o\")\n",
    "            print(\"   Recomendaci√≥n: Eliminar y volver a procesar con OCR\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Diagnosticar bases3.pdf que parece tener problemas\n",
    "diagnose_pdf_issues(\"bases3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a088f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 14: LIMPIEZA Y REPROCESAMIENTO FORZADO\n",
    "# ============================================================================\n",
    "\n",
    "def force_reprocess_problematic_files():\n",
    "    \"\"\"\n",
    "    Reprocesa archivos que tienen 0 caracteres o an√°lisis vac√≠os.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîß REPROCESAMIENTO FORZADO DE ARCHIVOS PROBLEM√ÅTICOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    problematic = []\n",
    "    \n",
    "    # Identificar archivos problem√°ticos\n",
    "    for pdf_name in [\"bases1.pdf\", \"bases2.pdf\", \"bases3.pdf\"]:\n",
    "        pdf_path = BASES_DIR / pdf_name\n",
    "        text_file = RESULTS_DIR / f\"{pdf_path.stem}_texto.txt\"\n",
    "        \n",
    "        if text_file.exists():\n",
    "            with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            if len(text) < 100:  # Menos de 100 caracteres es problem√°tico\n",
    "                problematic.append(pdf_name)\n",
    "                print(f\"   ‚ö†Ô∏è {pdf_name}: Solo {len(text)} caracteres\")\n",
    "    \n",
    "    if not problematic:\n",
    "        print(\"   ‚úÖ No se encontraron archivos problem√°ticos\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n   Archivos a reprocesar: {', '.join(problematic)}\")\n",
    "    \n",
    "    for pdf_name in problematic:\n",
    "        print(f\"\\nüîÑ Reprocesando {pdf_name}...\")\n",
    "        \n",
    "        # Eliminar archivos antiguos\n",
    "        pdf_path = BASES_DIR / pdf_name\n",
    "        text_file = RESULTS_DIR / f\"{pdf_path.stem}_texto.txt\"\n",
    "        analysis_file = RESULTS_DIR / f\"{pdf_path.stem}_analisis_completo.json\"\n",
    "        \n",
    "        if text_file.exists():\n",
    "            text_file.unlink()\n",
    "            print(f\"   üóëÔ∏è Eliminado texto anterior\")\n",
    "        \n",
    "        if analysis_file.exists():\n",
    "            analysis_file.unlink()\n",
    "            print(f\"   üóëÔ∏è Eliminado an√°lisis anterior\")\n",
    "        \n",
    "        # Reprocesar\n",
    "        result = analyze_pdf_document(pdf_path, force_ocr=True)\n",
    "        \n",
    "        if result['success'] and result['extraction']['total_characters'] > 100:\n",
    "            print(f\"   ‚úÖ Reprocesamiento exitoso: {result['extraction']['total_characters']:,} caracteres\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Fall√≥ el reprocesamiento\")\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de reprocesamiento forzado cargada\")\n",
    "print(\"\\nUSO: force_reprocess_problematic_files()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELDA 15: EJECUTOR MAESTRO DEL SISTEMA\n",
    "# ============================================================================\n",
    "\n",
    "def run_complete_system_analysis():\n",
    "    \"\"\"\n",
    "    Ejecuta el an√°lisis completo del sistema con todas las optimizaciones.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ SISTEMA MOP ANALYZER v2.0 - AN√ÅLISIS COMPLETO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # PASO 1: Verificaci√≥n inicial\n",
    "    print(\"\\nüìÅ PASO 1: Verificaci√≥n del sistema\")\n",
    "    print(\"-\" * 40)\n",
    "    check_existing_files()\n",
    "    \n",
    "    # PASO 2: Procesar todos los documentos\n",
    "    print(\"\\nüìÑ PASO 2: Procesamiento paralelo de documentos\")\n",
    "    print(\"-\" * 40)\n",
    "    results = process_all_bases_complete()\n",
    "    \n",
    "    # PASO 3: Generar informe consolidado\n",
    "    print(\"\\nüìä PASO 3: Generaci√≥n de informes\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if results:\n",
    "        report = generate_consolidated_report()\n",
    "        \n",
    "        # Estad√≠sticas finales\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ AN√ÅLISIS COMPLETO FINALIZADO\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"   ‚è±Ô∏è Tiempo total: {elapsed:.1f} segundos ({elapsed/60:.1f} minutos)\")\n",
    "        print(f\"   üìÑ Documentos procesados: {len(results)}\")\n",
    "        print(f\"   üìä Informe disponible en: {report}\")\n",
    "        print(f\"   üöÄ Velocidad promedio: {elapsed/len(results):.1f}s por documento\")\n",
    "    else:\n",
    "        print(\"‚ùå No se procesaron documentos\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ SISTEMA MOP ANALYZER v2.0 - CARGADO COMPLETAMENTE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüéØ COMANDOS PRINCIPALES:\")\n",
    "print(\"\\n1. AN√ÅLISIS COMPLETO DE TODOS LOS PDFs:\")\n",
    "print(\"   >>> run_complete_system_analysis()\")\n",
    "print(\"\\n2. Procesar todos los archivos:\")\n",
    "print(\"   >>> process_all_bases_complete()\")\n",
    "print(\"\\n3. Reprocesar archivo con m√°s workers:\")\n",
    "print(\"   >>> reprocess_file_enhanced('bases3.pdf', max_workers=6, chunk_size=10)\")\n",
    "print(\"\\n4. Verificar estado:\")\n",
    "print(\"   >>> check_existing_files()\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec179756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el an√°lisis completo del sistema\n",
    "run_complete_system_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c118db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERAR INFORME HTML DESDE DATOS EXISTENTES\n",
    "# ============================================================================\n",
    "\n",
    "def generate_html_from_results(results_data):\n",
    "    \"\"\"\n",
    "    Genera un informe HTML desde los datos de resultados ya procesados.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä GENERANDO INFORME HTML DESDE DATOS EXISTENTES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not results_data:\n",
    "        print(\"‚ùå No hay datos para generar el informe\")\n",
    "        return None\n",
    "    \n",
    "    # Estad√≠sticas globales\n",
    "    total_docs = len(results_data)\n",
    "    total_chars = sum(data['extraction']['total_characters'] for data in results_data)\n",
    "    total_mop = sum(data['summary'].get('codigos_mop', 0) for data in results_data)\n",
    "    total_items = sum(data['summary'].get('items_principales', 0) for data in results_data)\n",
    "    total_ete = sum(data['summary'].get('codigos_ete', 0) for data in results_data)\n",
    "    total_safi = sum(data['summary'].get('codigos_safi', 0) for data in results_data)\n",
    "    \n",
    "    # Crear HTML completo\n",
    "    html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>üìä Informe Consolidado MOP - An√°lisis de Presupuestos</title>\n",
    "    <style>\n",
    "        * {{\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            box-sizing: border-box;\n",
    "        }}\n",
    "        \n",
    "        body {{\n",
    "            font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, Arial, sans-serif;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            min-height: 100vh;\n",
    "            padding: 20px;\n",
    "            line-height: 1.6;\n",
    "        }}\n",
    "        \n",
    "        .container {{\n",
    "            max-width: 1400px;\n",
    "            margin: 0 auto;\n",
    "            background: white;\n",
    "            border-radius: 20px;\n",
    "            box-shadow: 0 20px 60px rgba(0,0,0,0.3);\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        \n",
    "        .header {{\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 40px;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        \n",
    "        .header h1 {{\n",
    "            font-size: 2.8em;\n",
    "            margin-bottom: 15px;\n",
    "            font-weight: 300;\n",
    "            text-shadow: 0 2px 4px rgba(0,0,0,0.3);\n",
    "        }}\n",
    "        \n",
    "        .header .subtitle {{\n",
    "            font-size: 1.3em;\n",
    "            opacity: 0.9;\n",
    "            margin-bottom: 20px;\n",
    "        }}\n",
    "        \n",
    "        .header .date {{\n",
    "            font-size: 1em;\n",
    "            opacity: 0.8;\n",
    "            background: rgba(255,255,255,0.1);\n",
    "            padding: 10px 20px;\n",
    "            border-radius: 25px;\n",
    "            display: inline-block;\n",
    "        }}\n",
    "        \n",
    "        .content {{\n",
    "            padding: 40px;\n",
    "        }}\n",
    "        \n",
    "        .summary-section {{\n",
    "            margin-bottom: 50px;\n",
    "        }}\n",
    "        \n",
    "        .section-title {{\n",
    "            font-size: 2em;\n",
    "            color: #333;\n",
    "            margin-bottom: 30px;\n",
    "            text-align: center;\n",
    "            position: relative;\n",
    "        }}\n",
    "        \n",
    "        .section-title::after {{\n",
    "            content: '';\n",
    "            position: absolute;\n",
    "            bottom: -10px;\n",
    "            left: 50%;\n",
    "            transform: translateX(-50%);\n",
    "            width: 60px;\n",
    "            height: 3px;\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "            border-radius: 2px;\n",
    "        }}\n",
    "        \n",
    "        .summary-grid {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "            gap: 25px;\n",
    "            margin: 40px 0;\n",
    "        }}\n",
    "        \n",
    "        .summary-card {{\n",
    "            background: white;\n",
    "            border-radius: 15px;\n",
    "            padding: 25px;\n",
    "            text-align: center;\n",
    "            box-shadow: 0 8px 25px rgba(0,0,0,0.1);\n",
    "            transition: all 0.3s ease;\n",
    "            border: 2px solid transparent;\n",
    "            position: relative;\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        \n",
    "        .summary-card::before {{\n",
    "            content: '';\n",
    "            position: absolute;\n",
    "            top: 0;\n",
    "            left: 0;\n",
    "            right: 0;\n",
    "            height: 4px;\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "        }}\n",
    "        \n",
    "        .summary-card:hover {{\n",
    "            transform: translateY(-5px);\n",
    "            box-shadow: 0 15px 35px rgba(0,0,0,0.15);\n",
    "        }}\n",
    "        \n",
    "        .summary-value {{\n",
    "            font-size: 2.2em;\n",
    "            font-weight: 700;\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "            -webkit-background-clip: text;\n",
    "            -webkit-text-fill-color: transparent;\n",
    "            background-clip: text;\n",
    "            margin-bottom: 8px;\n",
    "        }}\n",
    "        \n",
    "        .summary-label {{\n",
    "            color: #666;\n",
    "            font-size: 0.95em;\n",
    "            font-weight: 500;\n",
    "            text-transform: uppercase;\n",
    "            letter-spacing: 1px;\n",
    "        }}\n",
    "        \n",
    "        .documents-table {{\n",
    "            width: 100%;\n",
    "            background: white;\n",
    "            border-radius: 15px;\n",
    "            overflow: hidden;\n",
    "            box-shadow: 0 8px 25px rgba(0,0,0,0.1);\n",
    "            margin-top: 30px;\n",
    "        }}\n",
    "        \n",
    "        .documents-table th {{\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "            color: white;\n",
    "            padding: 18px 12px;\n",
    "            text-align: left;\n",
    "            font-weight: 600;\n",
    "            text-transform: uppercase;\n",
    "            letter-spacing: 0.5px;\n",
    "            font-size: 0.85em;\n",
    "        }}\n",
    "        \n",
    "        .documents-table td {{\n",
    "            padding: 15px 12px;\n",
    "            border-bottom: 1px solid #eee;\n",
    "            vertical-align: top;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        \n",
    "        .documents-table tr:last-child td {{\n",
    "            border-bottom: none;\n",
    "        }}\n",
    "        \n",
    "        .documents-table tbody tr:hover {{\n",
    "            background: #f8f9ff;\n",
    "        }}\n",
    "        \n",
    "        .doc-name {{\n",
    "            font-weight: 600;\n",
    "            color: #333;\n",
    "            font-size: 1em;\n",
    "        }}\n",
    "        \n",
    "        .project-name {{\n",
    "            color: #555;\n",
    "            max-width: 250px;\n",
    "            word-wrap: break-word;\n",
    "            line-height: 1.4;\n",
    "        }}\n",
    "        \n",
    "        .no-data {{\n",
    "            color: #999;\n",
    "            font-style: italic;\n",
    "        }}\n",
    "        \n",
    "        .badge {{\n",
    "            padding: 4px 10px;\n",
    "            border-radius: 15px;\n",
    "            font-size: 0.8em;\n",
    "            font-weight: 600;\n",
    "            display: inline-block;\n",
    "            margin: 2px;\n",
    "        }}\n",
    "        \n",
    "        .badge-mop {{\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "            color: white;\n",
    "        }}\n",
    "        \n",
    "        .badge-ete {{\n",
    "            background: #28a745;\n",
    "            color: white;\n",
    "        }}\n",
    "        \n",
    "        .badge-safi {{\n",
    "            background: #ffc107;\n",
    "            color: #333;\n",
    "        }}\n",
    "        \n",
    "        .chars-count {{\n",
    "            color: #666;\n",
    "            font-size: 0.85em;\n",
    "        }}\n",
    "        \n",
    "        .location {{\n",
    "            color: #555;\n",
    "            font-size: 0.9em;\n",
    "            line-height: 1.3;\n",
    "        }}\n",
    "        \n",
    "        .presupuesto {{\n",
    "            color: #28a745;\n",
    "            font-weight: 600;\n",
    "        }}\n",
    "        \n",
    "        .footer {{\n",
    "            text-align: center;\n",
    "            padding: 30px;\n",
    "            background: #f8f9fa;\n",
    "            color: #666;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        \n",
    "        .codes-section {{\n",
    "            margin-top: 40px;\n",
    "        }}\n",
    "        \n",
    "        .codes-grid {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n",
    "            gap: 25px;\n",
    "            margin: 30px 0;\n",
    "        }}\n",
    "        \n",
    "        .codes-card {{\n",
    "            background: white;\n",
    "            border-radius: 12px;\n",
    "            padding: 20px;\n",
    "            box-shadow: 0 4px 15px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        \n",
    "        .codes-title {{\n",
    "            font-weight: 600;\n",
    "            color: #333;\n",
    "            margin-bottom: 15px;\n",
    "            font-size: 1.1em;\n",
    "        }}\n",
    "        \n",
    "        .code-item {{\n",
    "            background: #f8f9fa;\n",
    "            padding: 8px 12px;\n",
    "            margin: 5px 0;\n",
    "            border-radius: 8px;\n",
    "            font-family: 'Courier New', monospace;\n",
    "            font-size: 0.9em;\n",
    "            border-left: 3px solid #667eea;\n",
    "        }}\n",
    "        \n",
    "        @media (max-width: 768px) {{\n",
    "            .header h1 {{ font-size: 2em; }}\n",
    "            .summary-grid {{ grid-template-columns: repeat(2, 1fr); }}\n",
    "            .content {{ padding: 20px; }}\n",
    "            .documents-table {{ font-size: 0.8em; }}\n",
    "            .documents-table th, .documents-table td {{ padding: 10px 6px; }}\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>üìä Informe Consolidado MOP</h1>\n",
    "            <div class=\"subtitle\">An√°lisis de Bases de Licitaci√≥n y Presupuestos</div>\n",
    "            <div class=\"date\">üìÖ Generado: {datetime.now().strftime('%d de %B de %Y a las %H:%M hrs')}</div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"content\">\n",
    "            <div class=\"summary-section\">\n",
    "                <h2 class=\"section-title\">üìà Resumen Ejecutivo</h2>\n",
    "                <div class=\"summary-grid\">\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{total_docs}</div>\n",
    "                        <div class=\"summary-label\">üìÑ Documentos</div>\n",
    "                    </div>\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{total_chars:,}</div>\n",
    "                        <div class=\"summary-label\">üìù Caracteres</div>\n",
    "                    </div>\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{total_mop}</div>\n",
    "                        <div class=\"summary-label\">üî¢ C√≥digos MOP</div>\n",
    "                    </div>\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{total_ete}</div>\n",
    "                        <div class=\"summary-label\">üìã C√≥digos ETE</div>\n",
    "                    </div>\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{total_safi}</div>\n",
    "                        <div class=\"summary-label\">üíº C√≥digos SAFI</div>\n",
    "                    </div>\n",
    "                    <div class=\"summary-card\">\n",
    "                        <div class=\"summary-value\">{total_items}</div>\n",
    "                        <div class=\"summary-label\">üìä Items Presup.</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"documents-section\">\n",
    "                <h2 class=\"section-title\">üìã Detalle por Documento</h2>\n",
    "                <table class=\"documents-table\">\n",
    "                    <thead>\n",
    "                        <tr>\n",
    "                            <th>üìÑ Archivo</th>\n",
    "                            <th>üèóÔ∏è Proyecto</th>\n",
    "                            <th>üìç Ubicaci√≥n</th>\n",
    "                            <th>üîß Tipo de Obra</th>\n",
    "                            <th>üí∞ Presupuesto</th>\n",
    "                            <th>üî¢ C√≥digos</th>\n",
    "                            <th>üìù Contenido</th>\n",
    "                        </tr>\n",
    "                    </thead>\n",
    "                    <tbody>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Agregar filas de documentos\n",
    "    for data in sorted(results_data, key=lambda x: x['filename']):\n",
    "        s = data['summary']\n",
    "        \n",
    "        # Procesar datos\n",
    "        comunas = ', '.join(s.get('comunas', [])[:2]) if s.get('comunas') else 'N/D'\n",
    "        if len(s.get('comunas', [])) > 2:\n",
    "            comunas += f\" (+{len(s.get('comunas', [])) - 2})\"\n",
    "        \n",
    "        proyecto = s.get('proyecto', 'No identificado')\n",
    "        if proyecto == 'No identificado':\n",
    "            proyecto = '<span class=\"no-data\">No identificado</span>'\n",
    "        else:\n",
    "            proyecto = proyecto[:70] + ('...' if len(proyecto) > 70 else '')\n",
    "        \n",
    "        region = s.get('region', 'N/D')\n",
    "        tipo_obra = s.get('tipo_obra', 'N/D')\n",
    "        if tipo_obra != 'N/D' and len(tipo_obra) > 35:\n",
    "            tipo_obra = tipo_obra[:35] + '...'\n",
    "        \n",
    "        presupuesto = s.get('presupuesto_estimado', 'N/D')\n",
    "        if presupuesto == 'N/D':\n",
    "            presupuesto = '<span class=\"no-data\">N/D</span>'\n",
    "        else:\n",
    "            presupuesto = f'<span class=\"presupuesto\">{presupuesto}</span>'\n",
    "        \n",
    "        mop_count = s.get('codigos_mop', 0)\n",
    "        ete_count = s.get('codigos_ete', 0)\n",
    "        safi_count = s.get('codigos_safi', 0)\n",
    "        chars_count = data['extraction']['total_characters']\n",
    "        \n",
    "        # Construir badges de c√≥digos\n",
    "        codes_html = \"\"\n",
    "        if mop_count > 0:\n",
    "            codes_html += f'<span class=\"badge badge-mop\">MOP: {mop_count}</span>'\n",
    "        if ete_count > 0:\n",
    "            codes_html += f'<span class=\"badge badge-ete\">ETE: {ete_count}</span>'\n",
    "        if safi_count > 0:\n",
    "            codes_html += f'<span class=\"badge badge-safi\">SAFI: {safi_count}</span>'\n",
    "        \n",
    "        if not codes_html:\n",
    "            codes_html = '<span class=\"no-data\">Sin c√≥digos</span>'\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "                        <tr>\n",
    "                            <td><div class=\"doc-name\">{data['filename']}</div></td>\n",
    "                            <td><div class=\"project-name\">{proyecto}</div></td>\n",
    "                            <td><div class=\"location\">{region}<br><small>{comunas}</small></div></td>\n",
    "                            <td>{tipo_obra}</td>\n",
    "                            <td>{presupuesto}</td>\n",
    "                            <td>{codes_html}</td>\n",
    "                            <td><div class=\"chars-count\">{chars_count:,} chars<br>{data['extraction'].get('total_words', 0):,} palabras</div></td>\n",
    "                        </tr>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Agregar secci√≥n de c√≥digos detallados\n",
    "    html_content += \"\"\"\n",
    "                    </tbody>\n",
    "                </table>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"codes-section\">\n",
    "                <h2 class=\"section-title\">üîç C√≥digos Identificados por Documento</h2>\n",
    "                <div class=\"codes-grid\">\n",
    "    \"\"\"\n",
    "    \n",
    "    for data in results_data:\n",
    "        patterns = data.get('patterns_extracted', {})\n",
    "        filename = data['filename']\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "                    <div class=\"codes-card\">\n",
    "                        <div class=\"codes-title\">üìÑ {filename}</div>\n",
    "        \"\"\"\n",
    "        \n",
    "        if patterns.get('mop_codes'):\n",
    "            html_content += f\"\"\"\n",
    "                        <div style=\"margin-bottom: 15px;\">\n",
    "                            <strong>üî¢ C√≥digos MOP ({len(patterns['mop_codes'])}):</strong>\n",
    "            \"\"\"\n",
    "            for code in patterns['mop_codes'][:10]:  # Mostrar m√°ximo 10\n",
    "                html_content += f'<div class=\"code-item\">{code}</div>'\n",
    "            if len(patterns['mop_codes']) > 10:\n",
    "                html_content += f'<div class=\"no-data\">... y {len(patterns[\"mop_codes\"]) - 10} m√°s</div>'\n",
    "            html_content += '</div>'\n",
    "        \n",
    "        if patterns.get('ete_codes'):\n",
    "            html_content += f\"\"\"\n",
    "                        <div style=\"margin-bottom: 15px;\">\n",
    "                            <strong>üìã C√≥digos ETE ({len(patterns['ete_codes'])}):</strong>\n",
    "            \"\"\"\n",
    "            for code in patterns['ete_codes']:\n",
    "                html_content += f'<div class=\"code-item\">{code}</div>'\n",
    "            html_content += '</div>'\n",
    "        \n",
    "        if patterns.get('safi_codes'):\n",
    "            html_content += f\"\"\"\n",
    "                        <div style=\"margin-bottom: 15px;\">\n",
    "                            <strong>üíº C√≥digos SAFI ({len(patterns['safi_codes'])}):</strong>\n",
    "            \"\"\"\n",
    "            for code in patterns['safi_codes']:\n",
    "                html_content += f'<div class=\"code-item\">{code}</div>'\n",
    "            html_content += '</div>'\n",
    "        \n",
    "        html_content += '</div>'\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"footer\">\n",
    "            <p>ü§ñ Generado autom√°ticamente por MOP Analyzer v2.0</p>\n",
    "            <p>Sistema de an√°lisis de documentos de licitaci√≥n del Ministerio de Obras P√∫blicas</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Guardar archivo\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_file = RESULTS_DIR / f\"informe_consolidado_{timestamp}.html\"\n",
    "    \n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    # Tambi√©n crear una versi√≥n sin timestamp\n",
    "    report_file_simple = RESULTS_DIR / \"informe_consolidado.html\"\n",
    "    with open(report_file_simple, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"\\n‚úÖ INFORME HTML GENERADO EXITOSAMENTE\")\n",
    "    print(f\"üìÅ Archivo con timestamp: {report_file}\")\n",
    "    print(f\"üìÅ Archivo principal: {report_file_simple}\")\n",
    "    print(f\"üåê Para ver el informe, abre cualquiera de los archivos HTML en tu navegador\")\n",
    "    \n",
    "    # Mostrar estad√≠sticas finales\n",
    "    print(f\"\\nüìä ESTAD√çSTICAS DEL INFORME:\")\n",
    "    print(f\"   üìÑ Documentos procesados: {total_docs}\")\n",
    "    print(f\"   üìù Total caracteres: {total_chars:,}\")\n",
    "    print(f\"   üî¢ Total c√≥digos MOP: {total_mop}\")\n",
    "    print(f\"   üìã Total c√≥digos ETE: {total_ete}\")\n",
    "    print(f\"   üíº Total c√≥digos SAFI: {total_safi}\")\n",
    "    print(f\"   üìä Total items presupuestarios: {total_items}\")\n",
    "    \n",
    "    return {\n",
    "        'report_file': report_file,\n",
    "        'report_file_simple': report_file_simple,\n",
    "        'total_documents': total_docs,\n",
    "        'total_characters': total_chars,\n",
    "        'total_mop_codes': total_mop,\n",
    "        'total_ete_codes': total_ete,\n",
    "        'total_safi_codes': total_safi,\n",
    "        'total_items': total_items\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funci√≥n para generar HTML desde datos existentes cargada\")\n",
    "print(\"\\nüöÄ EJECUTAR:\")\n",
    "print(\"generate_html_from_results(results)\")\n",
    "\n",
    "# Ejecutar autom√°ticamente si tienes la variable 'results' disponible\n",
    "try:\n",
    "    # Intentar usar la variable 'results' del output anterior\n",
    "    if 'results' in locals() or 'results' in globals():\n",
    "        print(\"\\nüîÑ Detectados resultados existentes, generando informe...\")\n",
    "        generate_html_from_results(results)\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Para generar el informe, ejecuta:\")\n",
    "        print(\"generate_html_from_results(tus_datos_de_resultados)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Para generar el informe, ejecuta:\")\n",
    "    print(\"generate_html_from_results(tus_datos_de_resultados)\")\n",
    "    print(f\"Error detectado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c30267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_html_from_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05932f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aceaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f574af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuraci√≥n optimizada cargada\n",
      "   üìÅ Cache: storage/projects/conservacion_caminos/cache\n",
      "   ‚ö° Workers: 8\n",
      "   üìÑ Chunk size: 10 p√°ginas\n",
      "\n",
      "================================================================================\n",
      "‚úÖ SISTEMA OPTIMIZADO LISTO\n",
      "================================================================================\n",
      "\n",
      "Ejecuta:\n",
      "  >>> results = process_all_pdfs_fast()\n",
      "\n",
      "Esto deber√≠a completarse en menos de 10 minutos\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERSI√ìN OPTIMIZADA - PROCESADOR OCR PARA PDFS MOP\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import hashlib\n",
    "import pickle\n",
    "from functools import lru_cache\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN OPTIMIZADA\n",
    "# ============================================================================\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv(Path('.env') if Path('.env').exists() else Path('../.env'))\n",
    "\n",
    "PDF_REST_API_KEY = os.getenv('PDF_REST_API_KEY')\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if not PDF_REST_API_KEY:\n",
    "    raise ValueError(\"‚ùå PDF_REST_API_KEY no encontrada en .env\")\n",
    "\n",
    "# Configuraci√≥n de rutas\n",
    "BASE_DIR = Path(\"storage/projects/conservacion_caminos\")\n",
    "BASES_DIR = BASE_DIR / \"bases\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "TEMP_DIR = BASE_DIR / \"temp\"\n",
    "CACHE_DIR = BASE_DIR / \"cache\"  # Nuevo directorio de cach√©\n",
    "\n",
    "# Crear directorios\n",
    "for dir_path in [RESULTS_DIR, TEMP_DIR, CACHE_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cliente Anthropic (opcional - solo si necesitas an√°lisis IA)\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else None\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN OPTIMIZADA DEL SISTEMA\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'CHUNK_SIZE': 10,        # Reducido de 30 a 10 p√°ginas\n",
    "    'MAX_WORKERS': 8,        # Aumentado de 3-4 a 8 workers\n",
    "    'OCR_TIMEOUT': 120,      # Reducido de 300-600 a 120 segundos\n",
    "    'RETRY_COUNT': 1,        # Reducido de 2 a 1 reintento\n",
    "    'USE_CACHE': True,       # Activar cach√©\n",
    "    'PARALLEL_MODE': 'thread',  # 'thread' o 'process'\n",
    "    'BATCH_SIZE': 5,         # Procesar en batches\n",
    "    'MIN_VALID_TEXT': 500,   # M√≠nimo de caracteres\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n optimizada cargada\")\n",
    "print(f\"   üìÅ Cache: {CACHE_DIR}\")\n",
    "print(f\"   ‚ö° Workers: {CONFIG['MAX_WORKERS']}\")\n",
    "print(f\"   üìÑ Chunk size: {CONFIG['CHUNK_SIZE']} p√°ginas\")\n",
    "\n",
    "# ============================================================================\n",
    "# SISTEMA DE CACH√â INTELIGENTE\n",
    "# ============================================================================\n",
    "\n",
    "class SmartCache:\n",
    "    \"\"\"Sistema de cach√© para evitar reprocesar chunks.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Path):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.index_file = cache_dir / \"cache_index.json\"\n",
    "        self.index = self._load_index()\n",
    "    \n",
    "    def _load_index(self):\n",
    "        if self.index_file.exists():\n",
    "            with open(self.index_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_index(self):\n",
    "        with open(self.index_file, 'w') as f:\n",
    "            json.dump(self.index, f)\n",
    "    \n",
    "    def get_hash(self, file_path: Path, start_page: int, end_page: int) -> str:\n",
    "        \"\"\"Genera hash √∫nico para un chunk.\"\"\"\n",
    "        key = f\"{file_path.name}_{start_page}_{end_page}_{file_path.stat().st_mtime}\"\n",
    "        return hashlib.md5(key.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, hash_key: str) -> Optional[Dict]:\n",
    "        \"\"\"Recupera resultado cacheado si existe.\"\"\"\n",
    "        if hash_key in self.index:\n",
    "            cache_file = self.cache_dir / f\"{hash_key}.pkl\"\n",
    "            if cache_file.exists():\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "        return None\n",
    "    \n",
    "    def set(self, hash_key: str, data: Dict):\n",
    "        \"\"\"Guarda resultado en cach√©.\"\"\"\n",
    "        cache_file = self.cache_dir / f\"{hash_key}.pkl\"\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        self.index[hash_key] = time.time()\n",
    "        self._save_index()\n",
    "\n",
    "cache = SmartCache(CACHE_DIR)\n",
    "\n",
    "# ============================================================================\n",
    "# SPLITTER OPTIMIZADO\n",
    "# ============================================================================\n",
    "\n",
    "class OptimizedPDFSplitter:\n",
    "    \"\"\"Divisi√≥n optimizada de PDFs.\"\"\"\n",
    "    \n",
    "    def __init__(self, pages_per_chunk: int = 10):\n",
    "        self.pages_per_chunk = pages_per_chunk\n",
    "    \n",
    "    def split_pdf_fast(self, pdf_path: Path) -> List[Tuple[Path, int, int]]:\n",
    "        \"\"\"Divisi√≥n r√°pida de PDF sin escribir chunks intermedios si no es necesario.\"\"\"\n",
    "        chunks_info = []\n",
    "        \n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            total_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        print(f\"üìÑ PDF: {pdf_path.name} ({total_pages} p√°ginas)\")\n",
    "        \n",
    "        # Crear directorio para chunks\n",
    "        chunks_dir = TEMP_DIR / f\"{pdf_path.stem}_chunks\"\n",
    "        chunks_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Dividir en chunks m√°s peque√±os\n",
    "        for i, start in enumerate(range(0, total_pages, self.pages_per_chunk), 1):\n",
    "            end = min(start + self.pages_per_chunk, total_pages)\n",
    "            chunk_path = chunks_dir / f\"{pdf_path.stem}_chunk_{i:03d}.pdf\"\n",
    "            \n",
    "            # Solo crear el chunk si no est√° cacheado\n",
    "            cache_key = cache.get_hash(pdf_path, start, end)\n",
    "            if CONFIG['USE_CACHE'] and cache.get(cache_key):\n",
    "                print(f\"   üì¶ Chunk {i}: p√°ginas {start+1}-{end} [CACHEADO]\")\n",
    "            else:\n",
    "                # Crear chunk\n",
    "                with open(pdf_path, 'rb') as input_file:\n",
    "                    reader = PyPDF2.PdfReader(input_file)\n",
    "                    writer = PyPDF2.PdfWriter()\n",
    "                    \n",
    "                    for page_num in range(start, end):\n",
    "                        writer.add_page(reader.pages[page_num])\n",
    "                    \n",
    "                    with open(chunk_path, 'wb') as output_file:\n",
    "                        writer.write(output_file)\n",
    "                \n",
    "                print(f\"   üì¶ Chunk {i}: p√°ginas {start+1}-{end}\")\n",
    "            \n",
    "            chunks_info.append((chunk_path, start + 1, end, cache_key))\n",
    "        \n",
    "        return chunks_info\n",
    "\n",
    "# ============================================================================\n",
    "# OCR OPTIMIZADO CON BATCHING\n",
    "# ============================================================================\n",
    "\n",
    "def apply_ocr_optimized(chunk_info: Tuple) -> Dict[str, Any]:\n",
    "    \"\"\"OCR optimizado con cach√© y timeouts reducidos.\"\"\"\n",
    "    chunk_path, start_page, end_page, cache_key = chunk_info\n",
    "    \n",
    "    # Verificar cach√©\n",
    "    if CONFIG['USE_CACHE']:\n",
    "        cached = cache.get(cache_key)\n",
    "        if cached:\n",
    "            return cached\n",
    "    \n",
    "    result = {\n",
    "        \"chunk_name\": chunk_path.name,\n",
    "        \"pages\": (start_page, end_page),\n",
    "        \"success\": False,\n",
    "        \"text\": \"\",\n",
    "        \"error\": None,\n",
    "        \"processing_time\": 0,\n",
    "        \"characters\": 0\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Si el archivo no existe (porque estaba cacheado), retornar cach√© vac√≠o\n",
    "    if not chunk_path.exists():\n",
    "        result[\"error\"] = \"Chunk file not found (likely cached)\"\n",
    "        return result\n",
    "    \n",
    "    try:\n",
    "        # OCR con timeout reducido\n",
    "        ocr_url = \"https://api.pdfrest.com/pdf-with-ocr-text\"\n",
    "        \n",
    "        with open(chunk_path, 'rb') as file:\n",
    "            files = [('file', (chunk_path.name, file, 'application/pdf'))]\n",
    "            headers = {'Api-Key': PDF_REST_API_KEY}\n",
    "            payload = {\n",
    "                'output': f'ocr_{chunk_path.stem}',\n",
    "                'languages': 'Spanish'\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                ocr_url,\n",
    "                headers=headers,\n",
    "                data=payload,\n",
    "                files=files,\n",
    "                timeout=CONFIG['OCR_TIMEOUT']\n",
    "            )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            output_url = data.get('outputUrl')\n",
    "            \n",
    "            if output_url:\n",
    "                # Descargar y extraer texto\n",
    "                pdf_response = requests.get(output_url, timeout=30)\n",
    "                \n",
    "                if pdf_response.status_code == 200:\n",
    "                    # Guardar temporalmente\n",
    "                    temp_pdf = TEMP_DIR / f\"temp_{chunk_path.stem}.pdf\"\n",
    "                    with open(temp_pdf, 'wb') as f:\n",
    "                        f.write(pdf_response.content)\n",
    "                    \n",
    "                    # Extraer texto\n",
    "                    extract_url = \"https://api.pdfrest.com/extracted-text\"\n",
    "                    with open(temp_pdf, 'rb') as file:\n",
    "                        files = [('file', (temp_pdf.name, file, 'application/pdf'))]\n",
    "                        response = requests.post(\n",
    "                            extract_url, \n",
    "                            headers=headers, \n",
    "                            files=files, \n",
    "                            timeout=30\n",
    "                        )\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        text = response.json().get('fullText', '')\n",
    "                        text = re.sub(r'\\[pdfRest.*?\\]', '', text)\n",
    "                        \n",
    "                        result[\"success\"] = True\n",
    "                        result[\"text\"] = text\n",
    "                        result[\"characters\"] = len(text)\n",
    "                    \n",
    "                    # Limpiar temporal\n",
    "                    if temp_pdf.exists():\n",
    "                        temp_pdf.unlink()\n",
    "    \n",
    "    except requests.Timeout:\n",
    "        result[\"error\"] = f\"Timeout ({CONFIG['OCR_TIMEOUT']}s)\"\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = str(e)[:100]\n",
    "    \n",
    "    result[\"processing_time\"] = time.time() - start_time\n",
    "    \n",
    "    # Guardar en cach√© si fue exitoso\n",
    "    if CONFIG['USE_CACHE'] and result[\"success\"]:\n",
    "        cache.set(cache_key, result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESAMIENTO PARALELO OPTIMIZADO\n",
    "# ============================================================================\n",
    "\n",
    "def process_chunks_batch_parallel(chunks_info: List[Tuple], max_workers: int = None) -> Dict:\n",
    "    \"\"\"Procesamiento paralelo optimizado con batching.\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = CONFIG['MAX_WORKERS']\n",
    "    \n",
    "    print(f\"\\n‚ö° Procesando {len(chunks_info)} chunks con {max_workers} workers\")\n",
    "    \n",
    "    results = []\n",
    "    texts_by_page = {}\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    cached = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Procesar en batches para mejor control\n",
    "        batch_size = CONFIG['BATCH_SIZE']\n",
    "        \n",
    "        with tqdm(total=len(chunks_info), desc=\"Procesando chunks\") as pbar:\n",
    "            for i in range(0, len(chunks_info), batch_size):\n",
    "                batch = chunks_info[i:i+batch_size]\n",
    "                \n",
    "                # Enviar batch\n",
    "                futures = {\n",
    "                    executor.submit(apply_ocr_optimized, chunk): chunk \n",
    "                    for chunk in batch\n",
    "                }\n",
    "                \n",
    "                # Procesar resultados del batch\n",
    "                for future in as_completed(futures):\n",
    "                    chunk_info = futures[future]\n",
    "                    try:\n",
    "                        result = future.result(timeout=CONFIG['OCR_TIMEOUT'] + 10)\n",
    "                        results.append(result)\n",
    "                        \n",
    "                        if result[\"success\"]:\n",
    "                            start_page = result[\"pages\"][0]\n",
    "                            texts_by_page[start_page] = result[\"text\"]\n",
    "                            successful += 1\n",
    "                            \n",
    "                            # Verificar si vino de cach√©\n",
    "                            if result[\"processing_time\"] < 0.1:\n",
    "                                cached += 1\n",
    "                        else:\n",
    "                            failed += 1\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        failed += 1\n",
    "                        print(f\"‚ùå Error: {str(e)[:50]}\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    \n",
    "    # Consolidar texto en orden\n",
    "    consolidated_text = \"\"\n",
    "    for page_num in sorted(texts_by_page.keys()):\n",
    "        consolidated_text += f\"\\n\\n--- P√°ginas {page_num} ---\\n\"\n",
    "        consolidated_text += texts_by_page[page_num]\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nüìä Resultados:\")\n",
    "    print(f\"   ‚úÖ Exitosos: {successful}/{len(chunks_info)}\")\n",
    "    print(f\"   üíæ Desde cach√©: {cached}\")\n",
    "    print(f\"   ‚ùå Fallidos: {failed}\")\n",
    "    print(f\"   ‚è±Ô∏è Tiempo: {elapsed:.1f}s ({elapsed/len(chunks_info):.1f}s/chunk)\")\n",
    "    \n",
    "    return {\n",
    "        \"success\": successful > 0,\n",
    "        \"text\": consolidated_text,\n",
    "        \"chunks_processed\": len(chunks_info),\n",
    "        \"chunks_successful\": successful,\n",
    "        \"chunks_failed\": failed,\n",
    "        \"chunks_cached\": cached,\n",
    "        \"total_characters\": len(consolidated_text),\n",
    "        \"processing_time\": elapsed\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCI√ìN PRINCIPAL OPTIMIZADA\n",
    "# ============================================================================\n",
    "\n",
    "def process_pdf_fast(pdf_path: Path, skip_ai: bool = True) -> Dict:\n",
    "    \"\"\"Procesa un PDF de forma optimizada.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚ö° PROCESAMIENTO R√ÅPIDO: {pdf_path.name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    total_start = time.time()\n",
    "    \n",
    "    # Verificar texto existente\n",
    "    text_file = RESULTS_DIR / f\"{pdf_path.stem}_texto.txt\"\n",
    "    if text_file.exists() and not CONFIG.get('FORCE_REPROCESS', False):\n",
    "        print(f\"‚úÖ Texto ya existe: {text_file.name}\")\n",
    "        with open(text_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"text\": text,\n",
    "            \"cached\": True,\n",
    "            \"processing_time\": 0\n",
    "        }\n",
    "    \n",
    "    # Dividir PDF\n",
    "    splitter = OptimizedPDFSplitter(pages_per_chunk=CONFIG['CHUNK_SIZE'])\n",
    "    chunks_info = splitter.split_pdf_fast(pdf_path)\n",
    "    \n",
    "    # Procesar chunks en paralelo\n",
    "    result = process_chunks_batch_parallel(chunks_info)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        # Guardar texto\n",
    "        text = result[\"text\"]\n",
    "        with open(text_file, 'w', encoding='utf-8', errors='ignore') as f:\n",
    "            f.write(text)\n",
    "        print(f\"üíæ Guardado: {text_file.name}\")\n",
    "        \n",
    "        # Extraer patrones b√°sicos (r√°pido)\n",
    "        patterns = extract_patterns_quick(text)\n",
    "        \n",
    "        # Guardar resumen b√°sico\n",
    "        summary = {\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_characters\": len(text),\n",
    "            \"processing_time\": time.time() - total_start,\n",
    "            \"chunks_cached\": result.get(\"chunks_cached\", 0),\n",
    "            \"patterns\": {\n",
    "                \"mop_codes\": len(patterns.get('mop_codes', [])),\n",
    "                \"ete_codes\": len(patterns.get('ete_codes', [])),\n",
    "                \"montos\": len(patterns.get('montos', []))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        summary_file = RESULTS_DIR / f\"{pdf_path.stem}_resumen_rapido.json\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Completado en {time.time() - total_start:.1f}s\")\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"text\": text,\n",
    "            \"summary\": summary,\n",
    "            \"processing_time\": time.time() - total_start\n",
    "        }\n",
    "    \n",
    "    return {\"success\": False, \"error\": \"Fall√≥ el procesamiento\"}\n",
    "\n",
    "def extract_patterns_quick(text: str) -> Dict:\n",
    "    \"\"\"Extracci√≥n r√°pida de patrones sin regex complejos.\"\"\"\n",
    "    return {\n",
    "        'mop_codes': re.findall(r'7\\.\\d{3}\\.\\d+', text)[:100],  # Limitar resultados\n",
    "        'ete_codes': re.findall(r'ETE[\\.\\-\\s]?\\d+', text, re.IGNORECASE)[:50],\n",
    "        'montos': re.findall(r'\\$\\s*[\\d\\.,]+', text)[:100]\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESAMIENTO EN BATCH OPTIMIZADO\n",
    "# ============================================================================\n",
    "\n",
    "def process_all_pdfs_fast():\n",
    "    \"\"\"Procesa todos los PDFs de forma optimizada.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ö° PROCESAMIENTO BATCH OPTIMIZADO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Buscar PDFs\n",
    "    pdf_files = list(BASES_DIR.glob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(\"‚ùå No se encontraron PDFs\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üìö Archivos encontrados: {len(pdf_files)}\")\n",
    "    for pdf in pdf_files:\n",
    "        size_mb = pdf.stat().st_size / 1024 / 1024\n",
    "        print(f\"   - {pdf.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Procesar todos\n",
    "    results = []\n",
    "    for idx, pdf_path in enumerate(pdf_files, 1):\n",
    "        print(f\"\\n[{idx}/{len(pdf_files)}] Procesando: {pdf_path.name}\")\n",
    "        result = process_pdf_fast(pdf_path, skip_ai=True)\n",
    "        results.append(result)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(f\"   ‚úÖ {result['summary']['total_characters']:,} caracteres\")\n",
    "            print(f\"   ‚è±Ô∏è {result['processing_time']:.1f}s\")\n",
    "    \n",
    "    # Resumen final\n",
    "    total_time = time.time() - start_time\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"üìä RESUMEN FINAL\")\n",
    "    print(f\"=\"*80)\n",
    "    print(f\"‚úÖ Exitosos: {successful}/{len(pdf_files)}\")\n",
    "    print(f\"‚è±Ô∏è Tiempo total: {total_time:.1f}s\")\n",
    "    print(f\"‚ö° Promedio: {total_time/len(pdf_files):.1f}s por archivo\")\n",
    "    \n",
    "    # Generar tabla resumen\n",
    "    summary_data = []\n",
    "    for pdf, result in zip(pdf_files, results):\n",
    "        if result[\"success\"]:\n",
    "            summary_data.append({\n",
    "                'Archivo': pdf.name,\n",
    "                'Tama√±o MB': round(pdf.stat().st_size / 1024 / 1024, 1),\n",
    "                'Caracteres': result['summary']['total_characters'],\n",
    "                'C√≥digos MOP': result['summary']['patterns']['mop_codes'],\n",
    "                'Tiempo (s)': round(result['processing_time'], 1),\n",
    "                'Chunks Cache': result['summary'].get('chunks_cached', 0)\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        display(df)\n",
    "        \n",
    "        # Guardar Excel\n",
    "        excel_file = RESULTS_DIR / f\"resumen_optimizado_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx\"\n",
    "        df.to_excel(excel_file, index=False)\n",
    "        print(f\"\\nüíæ Resumen guardado: {excel_file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCI√ìN DE LIMPIEZA\n",
    "# ============================================================================\n",
    "\n",
    "def cleanup_temp_files():\n",
    "    \"\"\"Limpia archivos temporales.\"\"\"\n",
    "    print(\"üßπ Limpiando archivos temporales...\")\n",
    "    \n",
    "    # Limpiar chunks\n",
    "    for chunk_dir in TEMP_DIR.glob(\"*_chunks\"):\n",
    "        for file in chunk_dir.glob(\"*.pdf\"):\n",
    "            file.unlink()\n",
    "        chunk_dir.rmdir()\n",
    "    \n",
    "    # Limpiar PDFs temporales\n",
    "    for temp_pdf in TEMP_DIR.glob(\"temp_*.pdf\"):\n",
    "        temp_pdf.unlink()\n",
    "    \n",
    "    print(\"‚úÖ Limpieza completada\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ SISTEMA OPTIMIZADO LISTO\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nEjecuta:\")\n",
    "print(\"  >>> results = process_all_pdfs_fast()\")\n",
    "print(\"\\nEsto deber√≠a completarse en menos de 10 minutos\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a24b13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚ö° PROCESAMIENTO BATCH OPTIMIZADO\n",
      "================================================================================\n",
      "üìö Archivos encontrados: 3\n",
      "   - bases2.pdf (7.6 MB)\n",
      "   - bases3.pdf (17.3 MB)\n",
      "   - bases1.pdf (12.2 MB)\n",
      "\n",
      "[1/3] Procesando: bases2.pdf\n",
      "\n",
      "======================================================================\n",
      "‚ö° PROCESAMIENTO R√ÅPIDO: bases2.pdf\n",
      "======================================================================\n",
      "üìÑ PDF: bases2.pdf (150 p√°ginas)\n",
      "   üì¶ Chunk 1: p√°ginas 1-10\n",
      "   üì¶ Chunk 2: p√°ginas 11-20\n",
      "   üì¶ Chunk 3: p√°ginas 21-30\n",
      "   üì¶ Chunk 4: p√°ginas 31-40\n",
      "   üì¶ Chunk 5: p√°ginas 41-50\n",
      "   üì¶ Chunk 6: p√°ginas 51-60\n",
      "   üì¶ Chunk 7: p√°ginas 61-70\n",
      "   üì¶ Chunk 8: p√°ginas 71-80\n",
      "   üì¶ Chunk 9: p√°ginas 81-90\n",
      "   üì¶ Chunk 10: p√°ginas 91-100\n",
      "   üì¶ Chunk 11: p√°ginas 101-110\n",
      "   üì¶ Chunk 12: p√°ginas 111-120\n",
      "   üì¶ Chunk 13: p√°ginas 121-130\n",
      "   üì¶ Chunk 14: p√°ginas 131-140\n",
      "   üì¶ Chunk 15: p√°ginas 141-150\n",
      "\n",
      "‚ö° Procesando 15 chunks con 8 workers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce93e8f465347eb98cfede5a0e1984d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando chunks:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Resultados:\n",
      "   ‚úÖ Exitosos: 15/15\n",
      "   üíæ Desde cach√©: 0\n",
      "   ‚ùå Fallidos: 0\n",
      "   ‚è±Ô∏è Tiempo: 170.7s (11.4s/chunk)\n",
      "üíæ Guardado: bases2_texto.txt\n",
      "\n",
      "‚úÖ Completado en 171.2s\n",
      "   ‚úÖ 338,247 caracteres\n",
      "   ‚è±Ô∏è 171.2s\n",
      "\n",
      "[2/3] Procesando: bases3.pdf\n",
      "\n",
      "======================================================================\n",
      "‚ö° PROCESAMIENTO R√ÅPIDO: bases3.pdf\n",
      "======================================================================\n",
      "üìÑ PDF: bases3.pdf (135 p√°ginas)\n",
      "   üì¶ Chunk 1: p√°ginas 1-10\n",
      "   üì¶ Chunk 2: p√°ginas 11-20\n",
      "   üì¶ Chunk 3: p√°ginas 21-30\n",
      "   üì¶ Chunk 4: p√°ginas 31-40\n",
      "   üì¶ Chunk 5: p√°ginas 41-50\n",
      "   üì¶ Chunk 6: p√°ginas 51-60\n",
      "   üì¶ Chunk 7: p√°ginas 61-70\n",
      "   üì¶ Chunk 8: p√°ginas 71-80\n",
      "   üì¶ Chunk 9: p√°ginas 81-90\n",
      "   üì¶ Chunk 10: p√°ginas 91-100\n",
      "   üì¶ Chunk 11: p√°ginas 101-110\n",
      "   üì¶ Chunk 12: p√°ginas 111-120\n",
      "   üì¶ Chunk 13: p√°ginas 121-130\n",
      "   üì¶ Chunk 14: p√°ginas 131-135\n",
      "\n",
      "‚ö° Procesando 14 chunks con 8 workers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110fc6a7e98c4e8098343e2da9040308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando chunks:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Resultados:\n",
      "   ‚úÖ Exitosos: 14/14\n",
      "   üíæ Desde cach√©: 0\n",
      "   ‚ùå Fallidos: 0\n",
      "   ‚è±Ô∏è Tiempo: 205.7s (14.7s/chunk)\n",
      "üíæ Guardado: bases3_texto.txt\n",
      "\n",
      "‚úÖ Completado en 206.2s\n",
      "   ‚úÖ 308,486 caracteres\n",
      "   ‚è±Ô∏è 206.2s\n",
      "\n",
      "[3/3] Procesando: bases1.pdf\n",
      "\n",
      "======================================================================\n",
      "‚ö° PROCESAMIENTO R√ÅPIDO: bases1.pdf\n",
      "======================================================================\n",
      "üìÑ PDF: bases1.pdf (100 p√°ginas)\n",
      "   üì¶ Chunk 1: p√°ginas 1-10\n",
      "   üì¶ Chunk 2: p√°ginas 11-20\n",
      "   üì¶ Chunk 3: p√°ginas 21-30\n",
      "   üì¶ Chunk 4: p√°ginas 31-40\n",
      "   üì¶ Chunk 5: p√°ginas 41-50\n",
      "   üì¶ Chunk 6: p√°ginas 51-60\n",
      "   üì¶ Chunk 7: p√°ginas 61-70\n",
      "   üì¶ Chunk 8: p√°ginas 71-80\n",
      "   üì¶ Chunk 9: p√°ginas 81-90\n",
      "   üì¶ Chunk 10: p√°ginas 91-100\n",
      "\n",
      "‚ö° Procesando 10 chunks con 8 workers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee6b8425f144c58bbe263d3971b4c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando chunks:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Resultados:\n",
      "   ‚úÖ Exitosos: 9/10\n",
      "   üíæ Desde cach√©: 0\n",
      "   ‚ùå Fallidos: 1\n",
      "   ‚è±Ô∏è Tiempo: 138.5s (13.8s/chunk)\n",
      "üíæ Guardado: bases1_texto.txt\n",
      "\n",
      "‚úÖ Completado en 138.8s\n",
      "   ‚úÖ 200,890 caracteres\n",
      "   ‚è±Ô∏è 138.8s\n",
      "\n",
      "================================================================================\n",
      "üìä RESUMEN FINAL\n",
      "================================================================================\n",
      "‚úÖ Exitosos: 3/3\n",
      "‚è±Ô∏è Tiempo total: 516.2s\n",
      "‚ö° Promedio: 172.1s por archivo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Archivo</th>\n",
       "      <th>Tama√±o MB</th>\n",
       "      <th>Caracteres</th>\n",
       "      <th>C√≥digos MOP</th>\n",
       "      <th>Tiempo (s)</th>\n",
       "      <th>Chunks Cache</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bases2.pdf</td>\n",
       "      <td>7.6</td>\n",
       "      <td>338247</td>\n",
       "      <td>0</td>\n",
       "      <td>171.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bases3.pdf</td>\n",
       "      <td>17.3</td>\n",
       "      <td>308486</td>\n",
       "      <td>100</td>\n",
       "      <td>206.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bases1.pdf</td>\n",
       "      <td>12.2</td>\n",
       "      <td>200890</td>\n",
       "      <td>27</td>\n",
       "      <td>138.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Archivo  Tama√±o MB  Caracteres  C√≥digos MOP  Tiempo (s)  Chunks Cache\n",
       "0  bases2.pdf        7.6      338247            0       171.2             0\n",
       "1  bases3.pdf       17.3      308486          100       206.2             0\n",
       "2  bases1.pdf       12.2      200890           27       138.8             0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Resumen guardado: storage/projects/conservacion_caminos/results/resumen_optimizado_20250905_1219.xlsx\n"
     ]
    }
   ],
   "source": [
    "results = process_all_pdfs_fast()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f5a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ BUDGET ANALYZER MOP INTELIGENTE CARGADO\n",
      "================================================================================\n",
      "\n",
      "Funciones principales:\n",
      "  ‚Ä¢ analyze_mop_budgets_smart() - An√°lisis inteligente con detecci√≥n de tipo\n",
      "  ‚Ä¢ analyze_single_smart('bases1_texto.txt') - An√°lisis individual inteligente\n",
      "\n",
      "Funciones optimizadas:\n",
      "  ‚Ä¢ analyze_mop_budgets_optimized() - An√°lisis con rate limiting\n",
      "  ‚Ä¢ analyze_single_optimized('archivo.txt') - An√°lisis individual optimizado\n",
      "\n",
      "üß† CARACTER√çSTICAS INTELIGENTES:\n",
      "   - Detecci√≥n autom√°tica del tipo de documento\n",
      "   - An√°lisis r√°pido previo antes del an√°lisis completo\n",
      "   - Extracci√≥n mejorada de informaci√≥n del proyecto\n",
      "   - Manejo espec√≠fico seg√∫n tipo de documento\n",
      "   - Rate limiting autom√°tico\n",
      "\n",
      "üí° Recomendado: results = analyze_mop_budgets_smart()\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BUDGET ANALYZER - AN√ÅLISIS PRESUPUESTARIO MOP CON CLAUDE (OPTIMIZADO)\n",
    "# ============================================================================\n",
    "\n",
    "import anthropic\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "# Cliente Anthropic\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "# Modelo a usar (Claude Sonnet 4)\n",
    "MODEL = \"claude-3-5-haiku-20241022\"\n",
    "\n",
    "# Rate limiting\n",
    "TOKENS_PER_MINUTE_LIMIT = 25000  # L√≠mite conservador (5k menos que el m√°ximo)\n",
    "DELAY_BETWEEN_REQUESTS = 60  # 60 segundos entre requests pesados\n",
    "\n",
    "# ============================================================================\n",
    "# ANALIZADOR DE PRESUPUESTOS MOP OPTIMIZADO\n",
    "# ============================================================================\n",
    "\n",
    "class MOPBudgetAnalyzerOptimized:\n",
    "    \"\"\"\n",
    "    Analizador optimizado con rate limiting y manejo de tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client: anthropic.Anthropic, model: str = MODEL):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.expected_total = 718998624  # Total esperado para validaci√≥n\n",
    "        self.last_request_time = 0\n",
    "        self.tokens_used_this_minute = 0\n",
    "        self.minute_start = time.time()\n",
    "        \n",
    "    def _check_rate_limit(self, estimated_tokens: int):\n",
    "        \"\"\"\n",
    "        Verifica y espera si es necesario para respetar rate limits.\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Reset contador cada minuto\n",
    "        if current_time - self.minute_start > 60:\n",
    "            self.tokens_used_this_minute = 0\n",
    "            self.minute_start = current_time\n",
    "        \n",
    "        # Si exceder√≠a el l√≠mite, esperar\n",
    "        if self.tokens_used_this_minute + estimated_tokens > TOKENS_PER_MINUTE_LIMIT:\n",
    "            wait_time = 60 - (current_time - self.minute_start) + 5  # +5 segundos de buffer\n",
    "            print(f\"‚è≥ Rate limit alcanzado. Esperando {wait_time:.1f}s...\")\n",
    "            time.sleep(wait_time)\n",
    "            self.tokens_used_this_minute = 0\n",
    "            self.minute_start = time.time()\n",
    "        \n",
    "        # Delay adicional entre requests\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        if time_since_last < DELAY_BETWEEN_REQUESTS:\n",
    "            sleep_time = DELAY_BETWEEN_REQUESTS - time_since_last\n",
    "            print(f\"‚è≥ Esperando delay entre requests: {sleep_time:.1f}s\")\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "        self.tokens_used_this_minute += estimated_tokens\n",
    "\n",
    "    def create_optimized_prompt(self, text: str, filename: str) -> str:\n",
    "        \"\"\"\n",
    "        Crea un prompt inteligente que identifica el tipo de documento.\n",
    "        \"\"\"\n",
    "        # Truncar texto inteligentemente\n",
    "        max_chars = 50000\n",
    "        \n",
    "        if len(text) > max_chars:\n",
    "            # Buscar secciones importantes\n",
    "            lines = text.split('\\n')\n",
    "            important_lines = []\n",
    "            char_count = 0\n",
    "            \n",
    "            # Keywords ampliados para diferentes tipos de documentos MOP\n",
    "            keywords = [\n",
    "                'presupuesto', 'item', 'c√≥digo', 'mop', 'total', 'precio', 'cantidad', 'designaci√≥n',\n",
    "                'proyecto', 'conservaci√≥n', 'caminos', 'comunas', 'regi√≥n', 'provincia',\n",
    "                'especificaciones', 'bases', 'obras p√∫blicas', 'contrato', 'licitaci√≥n'\n",
    "            ]\n",
    "            \n",
    "            for line in lines[:2000]:  # Revisar m√°s l√≠neas\n",
    "                if char_count > max_chars:\n",
    "                    break\n",
    "                    \n",
    "                line_lower = line.lower()\n",
    "                if any(keyword in line_lower for keyword in keywords) or len(line) > 80:\n",
    "                    important_lines.append(line)\n",
    "                    char_count += len(line) + 1\n",
    "                elif len(important_lines) < 50:  # M√°s contexto\n",
    "                    important_lines.append(line)\n",
    "                    char_count += len(line) + 1\n",
    "            \n",
    "            text = '\\n'.join(important_lines)\n",
    "        \n",
    "        return f\"\"\"Analiza este documento MOP chileno e identifica toda la informaci√≥n del proyecto.\n",
    "\n",
    "ARCHIVO: {filename}\n",
    "\n",
    "DOCUMENTO:\n",
    "{text}\n",
    "\n",
    "INSTRUCCIONES:\n",
    "1. IDENTIFICA el tipo de documento (presupuesto, bases administrativas, especificaciones, etc.)\n",
    "2. EXTRAE informaci√≥n del proyecto: nombre completo, ubicaci√≥n, tipo de obra\n",
    "3. Si hay presupuesto: busca tablas con c√≥digos MOP (7.XXX.XXX) y totales\n",
    "4. Si es bases/especificaciones: extrae informaci√≥n t√©cnica del proyecto\n",
    "5. REGI√ìN esperada: Los R√≠os\n",
    "\n",
    "RESPONDE SOLO JSON:\n",
    "{{\n",
    "  \"tipo_documento\": \"presupuesto|bases_administrativas|especificaciones|otro\",\n",
    "  \"proyecto\": {{\n",
    "    \"nombre\": \"nombre completo extra√≠do del documento\",\n",
    "    \"region\": \"regi√≥n identificada\",\n",
    "    \"provincia\": \"provincia si se menciona\",\n",
    "    \"comunas\": [\"lista de comunas mencionadas\"],\n",
    "    \"tipo_obra\": \"tipo espec√≠fico de obra\",\n",
    "    \"etapa\": \"etapa del proyecto si se menciona\",\n",
    "    \"mandante\": \"entidad responsable\"\n",
    "  }},\n",
    "  \"presupuesto\": {{\n",
    "    \"tiene_datos_presupuestarios\": true,\n",
    "    \"total_neto\": 0,\n",
    "    \"iva\": 0,\n",
    "    \"total_con_iva\": 0,\n",
    "    \"moneda\": \"CLP\"\n",
    "  }},\n",
    "  \"items_presupuestarios\": [\n",
    "    {{\n",
    "      \"codigo_mop\": \"c√≥digo si existe\",\n",
    "      \"descripcion\": \"descripci√≥n del item\",\n",
    "      \"cantidad\": 0,\n",
    "      \"unidad\": \"unidad\",\n",
    "      \"precio_unitario\": 0,\n",
    "      \"total\": 0\n",
    "    }}\n",
    "  ],\n",
    "  \"especificaciones_tecnicas\": {{\n",
    "    \"participacion_ciudadana\": true,\n",
    "    \"gestion_calidad\": true,\n",
    "    \"otras_especificaciones\": [\"lista de especificaciones encontradas\"]\n",
    "  }},\n",
    "  \"metadata\": {{\n",
    "    \"items_extraidos\": 0,\n",
    "    \"confianza_extraccion\": 0.95,\n",
    "    \"observaciones\": [\"observaciones importantes\"]\n",
    "  }}\n",
    "}}\"\"\"\n",
    "    \n",
    "    def analyze_document_optimized(self, text_file: Path) -> Dict:\n",
    "        \"\"\"\n",
    "        Analiza un documento con rate limiting optimizado.\n",
    "        \"\"\"\n",
    "        print(f\"\\nü§ñ Analizando con Claude Sonnet 4 (optimizado): {text_file.name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Leer texto\n",
    "        with open(text_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Estad√≠sticas del texto\n",
    "        chars = len(text)\n",
    "        tokens_estimate = min(chars / 4, 20000)  # Limitamos estimaci√≥n\n",
    "        \n",
    "        print(f\"üìÑ Caracteres: {chars:,}\")\n",
    "        print(f\"üéØ Tokens estimados (limitados): {tokens_estimate:,.0f}\")\n",
    "        \n",
    "        # Verificar rate limit antes de proceder\n",
    "        self._check_rate_limit(int(tokens_estimate))\n",
    "        \n",
    "        # Crear prompt optimizado\n",
    "        prompt = self.create_optimized_prompt(text, text_file.name)\n",
    "        \n",
    "        try:\n",
    "            # Llamar a Claude con configuraci√≥n optimizada\n",
    "            print(\"‚è≥ Procesando con Claude...\")\n",
    "            response = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=4000,  # Reducido para controlar costos\n",
    "                temperature=0,     # M√°xima precisi√≥n\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            # Extraer respuesta\n",
    "            response_text = response.content[0].text\n",
    "            \n",
    "            # Parsear JSON con mejor error handling\n",
    "            json_start = response_text.find('{')\n",
    "            json_end = response_text.rfind('}') + 1\n",
    "            \n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_text = response_text[json_start:json_end]\n",
    "                \n",
    "                # Limpiar JSON com√∫n problemas\n",
    "                json_text = re.sub(r',\\s*}', '}', json_text)  # Comas finales\n",
    "                json_text = re.sub(r',\\s*]', ']', json_text)  # Comas en arrays\n",
    "                \n",
    "                try:\n",
    "                    analysis = json.loads(json_text)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ö†Ô∏è Error JSON: {e}\")\n",
    "                    print(f\"JSON problem√°tico: {json_text[:500]}...\")\n",
    "                    analysis = self._create_fallback_analysis(text, text_file.name)\n",
    "                \n",
    "                # Validar y enriquecer\n",
    "                analysis = self.validate_and_enrich(analysis)\n",
    "                \n",
    "                # Calcular costos con tokens reales del response\n",
    "                input_tokens = len(prompt) / 4\n",
    "                output_tokens = len(response_text) / 4\n",
    "                input_cost = (input_tokens / 1_000_000) * 3.0\n",
    "                output_cost = (output_tokens / 1_000_000) * 15.0\n",
    "                total_cost = input_cost + output_cost\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                print(f\"‚úÖ An√°lisis completado\")\n",
    "                print(f\"   ‚è±Ô∏è Tiempo: {elapsed:.1f}s\")\n",
    "                print(f\"   üí∞ Costo: ${total_cost:.4f}\")\n",
    "                print(f\"   üìä Items extra√≠dos: {len(analysis.get('items', []))}\")\n",
    "                \n",
    "                # Guardar resultado\n",
    "                output_file = RESULTS_DIR / f\"{text_file.stem}_analisis_optimized.json\"\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(analysis, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "                print(f\"   üíæ Guardado: {output_file.name}\")\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"analysis\": analysis,\n",
    "                    \"file\": text_file.name,\n",
    "                    \"cost\": total_cost,\n",
    "                    \"time\": elapsed,\n",
    "                    \"tokens\": {\"input\": input_tokens, \"output\": output_tokens}\n",
    "                }\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(\"No se pudo extraer JSON de la respuesta\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            \n",
    "            # Si es rate limit, dar informaci√≥n espec√≠fica\n",
    "            if \"rate_limit\" in str(e).lower():\n",
    "                print(\"üî¥ Rate limit alcanzado. Recomendaciones:\")\n",
    "                print(\"   - Esperar 1 minuto antes del pr√≥ximo an√°lisis\")\n",
    "                print(\"   - Reducir tama√±o de documentos\")\n",
    "                print(\"   - Usar analyze_batch_with_delays() para m√∫ltiples archivos\")\n",
    "            \n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"file\": text_file.name,\n",
    "                \"error_type\": \"rate_limit\" if \"rate_limit\" in str(e).lower() else \"processing\"\n",
    "            }\n",
    "    \n",
    "    def _create_fallback_analysis(self, text: str, filename: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Crea an√°lisis b√°sico cuando falla el parsing JSON usando patrones de texto.\n",
    "        \"\"\"\n",
    "        # Buscar informaci√≥n del proyecto en el texto\n",
    "        project_patterns = {\n",
    "            'conservacion': r'conservaci[o√≥]n.*?de.*?caminos',\n",
    "            'comunas': r'comunas?\\s+de\\s+([^,\\n]+)',\n",
    "            'region': r'regi[o√≥]n\\s+de\\s+([^,\\n]+)',\n",
    "            'provincia': r'provincia\\s+del?\\s+([^,\\n]+)'\n",
    "        }\n",
    "        \n",
    "        # Extraer informaci√≥n usando patrones\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Buscar nombre del proyecto\n",
    "        proyecto_nombre = \"Documento MOP\"\n",
    "        if 'conservaci√≥n' in text_lower and 'caminos' in text_lower:\n",
    "            if 'comunidades ind√≠genas' in text_lower:\n",
    "                proyecto_nombre = \"Conservaci√≥n de caminos de acceso a comunidades ind√≠genas\"\n",
    "        \n",
    "        # Buscar comunas\n",
    "        comunas = []\n",
    "        for match in re.finditer(r'comunas?\\s+de\\s+([^,\\n.]+)', text_lower):\n",
    "            comuna = match.group(1).strip().title()\n",
    "            if comuna not in comunas:\n",
    "                comunas.append(comuna)\n",
    "        \n",
    "        # Si no encuentra comunas espec√≠ficas, buscar nombres conocidos\n",
    "        if not comunas:\n",
    "            comunas_conocidas = ['Lago Ranco', 'Futrono', 'Valdivia', 'La Uni√≥n', 'R√≠o Bueno']\n",
    "            for comuna in comunas_conocidas:\n",
    "                if comuna.lower() in text_lower:\n",
    "                    comunas.append(comuna)\n",
    "        \n",
    "        # Determinar tipo de documento\n",
    "        tipo_doc = \"documento_mop\"\n",
    "        if 'presupuesto' in text_lower and 'precio' in text_lower:\n",
    "            tipo_doc = \"presupuesto\"\n",
    "        elif 'bases administrativas' in text_lower:\n",
    "            tipo_doc = \"bases_administrativas\"\n",
    "        elif 'especificaciones' in text_lower:\n",
    "            tipo_doc = \"especificaciones\"\n",
    "        \n",
    "        # Buscar c√≥digos MOP o items presupuestarios\n",
    "        items_encontrados = []\n",
    "        codigo_pattern = r'7\\.\\d{3}\\.\\d{3}'\n",
    "        codigos = re.findall(codigo_pattern, text)\n",
    "        \n",
    "        for codigo in codigos[:10]:  # Limitar a 10 items\n",
    "            items_encontrados.append({\n",
    "                \"codigo_mop\": codigo,\n",
    "                \"descripcion\": \"Item extra√≠do del documento\",\n",
    "                \"cantidad\": 0,\n",
    "                \"unidad\": \"ST\",\n",
    "                \"precio_unitario\": 0,\n",
    "                \"total\": 0\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"tipo_documento\": tipo_doc,\n",
    "            \"proyecto\": {\n",
    "                \"nombre\": proyecto_nombre,\n",
    "                \"region\": \"Los R√≠os\",\n",
    "                \"provincia\": \"Del Ranco\" if 'ranco' in text_lower else \"Los R√≠os\",\n",
    "                \"comunas\": comunas if comunas else [\"Por determinar\"],\n",
    "                \"tipo_obra\": \"Conservaci√≥n de caminos\",\n",
    "                \"etapa\": \"An√°lisis de documento\",\n",
    "                \"mandante\": \"MOP - Ministerio de Obras P√∫blicas\"\n",
    "            },\n",
    "            \"presupuesto\": {\n",
    "                \"tiene_datos_presupuestarios\": len(items_encontrados) > 0,\n",
    "                \"total_neto\": 0,\n",
    "                \"iva\": 0,\n",
    "                \"total_con_iva\": 0,\n",
    "                \"moneda\": \"CLP\"\n",
    "            },\n",
    "            \"items_presupuestarios\": items_encontrados,\n",
    "            \"especificaciones_tecnicas\": {\n",
    "                \"participacion_ciudadana\": \"participaci√≥n ciudadana\" in text_lower,\n",
    "                \"gestion_calidad\": \"gesti√≥n de la calidad\" in text_lower or \"calidad\" in text_lower,\n",
    "                \"otras_especificaciones\": self._extract_specifications(text_lower)\n",
    "            },\n",
    "            \"metadata\": {\n",
    "                \"items_extraidos\": len(items_encontrados),\n",
    "                \"confianza_extraccion\": 0.6,\n",
    "                \"es_fallback\": True,\n",
    "                \"observaciones\": [f\"An√°lisis fallback aplicado a {filename}\"]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _extract_specifications(self, text_lower: str) -> List[str]:\n",
    "        \"\"\"Extrae especificaciones t√©cnicas del texto.\"\"\"\n",
    "        specs = []\n",
    "        \n",
    "        spec_keywords = [\n",
    "            \"participaci√≥n ciudadana\",\n",
    "            \"gesti√≥n de la calidad\", \n",
    "            \"especificaciones ambientales\",\n",
    "            \"consulta ind√≠gena\",\n",
    "            \"bases administrativas\",\n",
    "            \"t√©rminos de referencia\"\n",
    "        ]\n",
    "        \n",
    "        for spec in spec_keywords:\n",
    "            if spec in text_lower:\n",
    "                specs.append(spec.title())\n",
    "        \n",
    "        return specs\n",
    "\n",
    "    def validate_and_enrich(self, analysis: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Valida y enriquece an√°lisis con la nueva estructura.\n",
    "        \"\"\"\n",
    "        # Convertir estructura nueva a formato compatible con reportes existentes\n",
    "        \n",
    "        # Validar estructura b√°sica\n",
    "        if not analysis.get(\"proyecto\"):\n",
    "            analysis[\"proyecto\"] = {\"nombre\": \"Proyecto no identificado\"}\n",
    "        \n",
    "        # Convertir items_presupuestarios a items (para compatibilidad)\n",
    "        items_presupuestarios = analysis.get(\"items_presupuestarios\", [])\n",
    "        analysis[\"items\"] = items_presupuestarios\n",
    "        \n",
    "        # Calcular total de items\n",
    "        total_calculado = sum(item.get('total', 0) for item in items_presupuestarios)\n",
    "        \n",
    "        # Enriquecer presupuesto\n",
    "        presupuesto = analysis.get(\"presupuesto\", {})\n",
    "        if total_calculado > 0:\n",
    "            presupuesto['total_calculado'] = total_calculado\n",
    "            presupuesto['total_con_iva'] = total_calculado\n",
    "        \n",
    "        # Validaci√≥n espec√≠fica\n",
    "        tiene_datos = presupuesto.get(\"tiene_datos_presupuestarios\", False)\n",
    "        if tiene_datos and total_calculado > 0:\n",
    "            diferencia = abs(total_calculado - self.expected_total)\n",
    "            pct_diferencia = (diferencia / self.expected_total) * 100 if self.expected_total > 0 else 100\n",
    "            \n",
    "            presupuesto['validacion'] = {\n",
    "                'total_esperado': self.expected_total,\n",
    "                'diferencia': diferencia,\n",
    "                'porcentaje_diferencia': round(pct_diferencia, 2),\n",
    "                'es_valido': pct_diferencia < 10\n",
    "            }\n",
    "        else:\n",
    "            # Para documentos sin presupuesto (bases, especificaciones)\n",
    "            presupuesto['validacion'] = {\n",
    "                'total_esperado': 0,\n",
    "                'diferencia': 0,\n",
    "                'porcentaje_diferencia': 0,\n",
    "                'es_valido': True  # V√°lido porque no es un documento presupuestario\n",
    "            }\n",
    "        \n",
    "        analysis[\"presupuesto\"] = presupuesto\n",
    "        \n",
    "        # Agregar metadata\n",
    "        metadata = analysis.get('metadata', {})\n",
    "        metadata['timestamp_analisis'] = datetime.now().isoformat()\n",
    "        metadata['modelo_usado'] = self.model\n",
    "        metadata['tipo_documento'] = analysis.get('tipo_documento', 'desconocido')\n",
    "        \n",
    "        analysis['metadata'] = metadata\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def analyze_batch_with_delays(self, text_files: List[Path]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analiza m√∫ltiples archivos con delays autom√°ticos entre cada uno.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        total_cost = 0\n",
    "        total_time = 0\n",
    "        \n",
    "        print(f\"\\nüöÄ AN√ÅLISIS BATCH DE {len(text_files)} ARCHIVOS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"‚è∞ Tiempo estimado: {len(text_files) * 2:.1f} minutos\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for i, text_file in enumerate(text_files, 1):\n",
    "            print(f\"\\nüìã Archivo {i}/{len(text_files)}\")\n",
    "            \n",
    "            result = self.analyze_document_optimized(text_file)\n",
    "            results.append(result)\n",
    "            \n",
    "            if result['success']:\n",
    "                total_cost += result['cost']\n",
    "                total_time += result['time']\n",
    "                \n",
    "                # Generar reporte HTML simplificado\n",
    "                html_report = self.generate_simple_html_report(result['analysis'])\n",
    "                \n",
    "                # Guardar HTML\n",
    "                html_file = RESULTS_DIR / f\"{text_file.stem}_reporte_optimized.html\"\n",
    "                with open(html_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(html_report)\n",
    "                \n",
    "                print(f\"   üìÑ Reporte HTML: {html_file.name}\")\n",
    "            \n",
    "            # Delay entre archivos (excepto el √∫ltimo)\n",
    "            if i < len(text_files):\n",
    "                print(f\"‚è≥ Esperando {DELAY_BETWEEN_REQUESTS}s antes del siguiente archivo...\")\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "        \n",
    "        # Resumen final\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"üìä RESUMEN BATCH\")\n",
    "        print(f\"=\"*60)\n",
    "        successful = len([r for r in results if r['success']])\n",
    "        print(f\"‚úÖ Exitosos: {successful}/{len(text_files)}\")\n",
    "        print(f\"‚è±Ô∏è Tiempo total: {total_time/60:.1f} min\")\n",
    "        print(f\"üí∞ Costo total: ${total_cost:.4f} USD\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def generate_simple_html_report(self, analysis: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Genera reporte HTML adaptado al tipo de documento.\n",
    "        \"\"\"\n",
    "        proyecto = analysis.get('proyecto', {})\n",
    "        presupuesto = analysis.get('presupuesto', {})\n",
    "        items = analysis.get('items', [])\n",
    "        tipo_doc = analysis.get('tipo_documento', 'documento_mop')\n",
    "        specs = analysis.get('especificaciones_tecnicas', {})\n",
    "        \n",
    "        # T√≠tulo seg√∫n tipo de documento\n",
    "        titulo_doc = {\n",
    "            'presupuesto': 'An√°lisis Presupuestario',\n",
    "            'bases_administrativas': 'An√°lisis de Bases Administrativas', \n",
    "            'especificaciones': 'An√°lisis de Especificaciones T√©cnicas',\n",
    "            'documento_mop': 'An√°lisis de Documento MOP'\n",
    "        }.get(tipo_doc, 'An√°lisis de Documento MOP')\n",
    "        \n",
    "        html = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{titulo_doc} - {proyecto.get('nombre', 'Proyecto')}</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}\n",
    "        .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; margin-bottom: 20px; }}\n",
    "        .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; background: #f8f9fa; }}\n",
    "        .presupuesto {{ background: #e8f5e8; }}\n",
    "        .especificaciones {{ background: #e8f0ff; }}\n",
    "        .warning {{ background: #fff3cd; border-color: #ffeaa7; }}\n",
    "        .total {{ font-size: 1.5em; color: #27ae60; font-weight: bold; }}\n",
    "        .no-presupuesto {{ font-size: 1.2em; color: #e67e22; font-weight: bold; }}\n",
    "        table {{ width: 100%; border-collapse: collapse; margin-top: 10px; }}\n",
    "        th, td {{ padding: 8px; border: 1px solid #ddd; text-align: left; }}\n",
    "        th {{ background: #f8f9fa; font-weight: bold; }}\n",
    "        .items-table {{ max-height: 400px; overflow-y: auto; }}\n",
    "        .badge {{ display: inline-block; padding: 3px 8px; border-radius: 3px; font-size: 0.9em; }}\n",
    "        .badge-success {{ background: #d4edda; color: #155724; }}\n",
    "        .badge-warning {{ background: #fff3cd; color: #856404; }}\n",
    "        .badge-info {{ background: #d1ecf1; color: #0c5460; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <h1>üìä {titulo_doc}</h1>\n",
    "        <p><strong>{proyecto.get('nombre', 'Proyecto MOP')}</strong></p>\n",
    "        <span class=\"badge badge-info\">Tipo: {tipo_doc.replace('_', ' ').title()}</span>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"section\">\n",
    "        <h2>üìç Informaci√≥n del Proyecto</h2>\n",
    "        <table>\n",
    "            <tr><td><strong>Nombre Completo:</strong></td><td>{proyecto.get('nombre', 'N/D')}</td></tr>\n",
    "            <tr><td><strong>Regi√≥n:</strong></td><td>{proyecto.get('region', 'N/D')}</td></tr>\n",
    "            <tr><td><strong>Provincia:</strong></td><td>{proyecto.get('provincia', 'N/D')}</td></tr>\n",
    "            <tr><td><strong>Comunas:</strong></td><td>{', '.join(proyecto.get('comunas', ['N/D']))}</td></tr>\n",
    "            <tr><td><strong>Tipo de Obra:</strong></td><td>{proyecto.get('tipo_obra', 'N/D')}</td></tr>\n",
    "            <tr><td><strong>Etapa:</strong></td><td>{proyecto.get('etapa', 'N/D')}</td></tr>\n",
    "            <tr><td><strong>Mandante:</strong></td><td>{proyecto.get('mandante', 'N/D')}</td></tr>\n",
    "        </table>\n",
    "    </div>\"\"\"\n",
    "        \n",
    "        # Secci√≥n de presupuesto (solo si hay datos)\n",
    "        tiene_presupuesto = presupuesto.get('tiene_datos_presupuestarios', False)\n",
    "        if tiene_presupuesto and len(items) > 0:\n",
    "            html += f\"\"\"\n",
    "    <div class=\"section presupuesto\">\n",
    "        <h2>üí∞ Informaci√≥n Presupuestaria</h2>\n",
    "        <p class=\"total\">Total Identificado: ${presupuesto.get('total_con_iva', 0):,.0f} CLP</p>\n",
    "        <table>\n",
    "            <tr><td><strong>Total Neto:</strong></td><td>${presupuesto.get('total_neto', 0):,.0f} CLP</td></tr>\n",
    "            <tr><td><strong>IVA (19%):</strong></td><td>${presupuesto.get('iva', 0):,.0f} CLP</td></tr>\n",
    "            <tr><td><strong>Total con IVA:</strong></td><td>${presupuesto.get('total_con_iva', 0):,.0f} CLP</td></tr>\n",
    "        </table>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"section\">\n",
    "        <h2>üìù Items Presupuestarios ({len(items)} items)</h2>\n",
    "        <div class=\"items-table\">\n",
    "            <table>\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th>C√≥digo MOP</th>\n",
    "                        <th>Descripci√≥n</th>\n",
    "                        <th>Cantidad</th>\n",
    "                        <th>Unidad</th>\n",
    "                        <th>P.Unitario</th>\n",
    "                        <th>Total</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\"\"\"\n",
    "            \n",
    "            for item in items[:30]:\n",
    "                html += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td>{item.get('codigo_mop', 'N/D')}</td>\n",
    "                        <td>{item.get('descripcion', item.get('designacion', 'N/D'))[:60]}...</td>\n",
    "                        <td>{item.get('cantidad', 0):,.2f}</td>\n",
    "                        <td>{item.get('unidad', 'N/D')}</td>\n",
    "                        <td>${item.get('precio_unitario', 0):,.0f}</td>\n",
    "                        <td>${item.get('total', 0):,.0f}</td>\n",
    "                    </tr>\"\"\"\n",
    "            \n",
    "            if len(items) > 30:\n",
    "                html += f\"\"\"\n",
    "                    <tr style=\"background: #fff3cd;\">\n",
    "                        <td colspan=\"6\" style=\"text-align: center;\">\n",
    "                            ‚ö†Ô∏è Mostrando 30 de {len(items)} items totales\n",
    "                        </td>\n",
    "                    </tr>\"\"\"\n",
    "            \n",
    "            html += \"\"\"\n",
    "                </tbody>\n",
    "            </table>\n",
    "        </div>\n",
    "    </div>\"\"\"\n",
    "        else:\n",
    "            html += f\"\"\"\n",
    "    <div class=\"section warning\">\n",
    "        <h2>üí∞ Informaci√≥n Presupuestaria</h2>\n",
    "        <p class=\"no-presupuesto\">Este documento no contiene datos presupuestarios detallados</p>\n",
    "        <p>Tipo de documento: <strong>{tipo_doc.replace('_', ' ').title()}</strong></p>\n",
    "        <p>Para an√°lisis presupuestario, se requiere el documento de presupuesto oficial del proyecto.</p>\n",
    "    </div>\"\"\"\n",
    "        \n",
    "        # Secci√≥n de especificaciones t√©cnicas\n",
    "        if specs:\n",
    "            html += f\"\"\"\n",
    "    <div class=\"section especificaciones\">\n",
    "        <h2>üìã Especificaciones T√©cnicas Identificadas</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <td><strong>Participaci√≥n Ciudadana:</strong></td>\n",
    "                <td>{'‚úÖ S√≠' if specs.get('participacion_ciudadana') else '‚ùå No'}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><strong>Gesti√≥n de Calidad:</strong></td>\n",
    "                <td>{'‚úÖ S√≠' if specs.get('gestion_calidad') else '‚ùå No'}</td>\n",
    "            </tr>\n",
    "        </table>\"\"\"\n",
    "            \n",
    "            otras_specs = specs.get('otras_especificaciones', [])\n",
    "            if otras_specs:\n",
    "                html += \"\"\"\n",
    "        <h3>Otras Especificaciones:</h3>\n",
    "        <ul>\"\"\"\n",
    "                for spec in otras_specs:\n",
    "                    html += f\"<li>{spec}</li>\"\n",
    "                html += \"</ul>\"\n",
    "            \n",
    "            html += \"</div>\"\n",
    "        \n",
    "        # Informaci√≥n del an√°lisis\n",
    "        metadata = analysis.get('metadata', {})\n",
    "        confianza = metadata.get('confianza_extraccion', metadata.get('confianza', 0))\n",
    "        \n",
    "        html += f\"\"\"\n",
    "    <div class=\"section\">\n",
    "        <h3>‚ÑπÔ∏è Informaci√≥n del An√°lisis</h3>\n",
    "        <table>\n",
    "            <tr><td><strong>Fecha:</strong></td><td>{datetime.now().strftime('%d/%m/%Y %H:%M')}</td></tr>\n",
    "            <tr><td><strong>Tipo de Documento:</strong></td><td>{tipo_doc.replace('_', ' ').title()}</td></tr>\n",
    "            <tr><td><strong>Items Extra√≠dos:</strong></td><td>{metadata.get('items_extraidos', len(items))}</td></tr>\n",
    "            <tr><td><strong>Confianza:</strong></td><td>{confianza*100:.1f}%</td></tr>\n",
    "            <tr><td><strong>M√©todo:</strong></td><td>{'An√°lisis Fallback' if metadata.get('es_fallback') else 'An√°lisis Claude'}</td></tr>\n",
    "        </table>\"\"\"\n",
    "        \n",
    "        observaciones = metadata.get('observaciones', [])\n",
    "        if observaciones:\n",
    "            html += \"<h4>Observaciones:</h4><ul>\"\n",
    "            for obs in observaciones:\n",
    "                html += f\"<li>{obs}</li>\"\n",
    "            html += \"</ul>\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "    </div>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        return html\n",
    "\n",
    "    def quick_document_analysis(self, text: str, filename: str) -> Dict:\n",
    "        \"\"\"\n",
    "        An√°lisis r√°pido para identificar tipo de documento y contenido b√°sico.\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Detectar tipo de documento\n",
    "        doc_type = \"documento_mop\"\n",
    "        if \"presupuesto oficial\" in text_lower or (\"precio\" in text_lower and \"total\" in text_lower and \"item\" in text_lower):\n",
    "            doc_type = \"presupuesto\"\n",
    "        elif \"bases administrativas\" in text_lower:\n",
    "            doc_type = \"bases_administrativas\"  \n",
    "        elif \"especificaciones\" in text_lower and (\"t√©cnicas\" in text_lower or \"ambientales\" in text_lower):\n",
    "            doc_type = \"especificaciones\"\n",
    "        \n",
    "        # Extraer informaci√≥n b√°sica del proyecto\n",
    "        proyecto_info = self._extract_project_info(text)\n",
    "        \n",
    "        # Buscar c√≥digos MOP\n",
    "        codigos_mop = re.findall(r'7\\.\\d{3}\\.\\d{3}', text)\n",
    "        \n",
    "        # Buscar totales monetarios\n",
    "        totales = re.findall(r'\\$?\\s*(\\d{1,3}(?:\\.\\d{3})+)', text)\n",
    "        totales_numericos = [int(t.replace('.', '')) for t in totales if len(t.replace('.', '')) >= 6]\n",
    "        \n",
    "        return {\n",
    "            \"tipo_documento\": doc_type,\n",
    "            \"proyecto_detectado\": proyecto_info,\n",
    "            \"codigos_mop_encontrados\": len(codigos_mop),\n",
    "            \"totales_monetarios\": totales_numericos[:5],  # Primeros 5 totales\n",
    "            \"tiene_datos_presupuestarios\": len(codigos_mop) > 0 or (doc_type == \"presupuesto\"),\n",
    "            \"confianza_deteccion\": self._calculate_confidence(doc_type, len(codigos_mop), proyecto_info)\n",
    "        }\n",
    "    \n",
    "    def _extract_project_info(self, text: str) -> Dict:\n",
    "        \"\"\"Extrae informaci√≥n b√°sica del proyecto del texto.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        info = {\n",
    "            \"nombre\": \"\",\n",
    "            \"region\": \"\",\n",
    "            \"comunas\": [],\n",
    "            \"tipo_obra\": \"\"\n",
    "        }\n",
    "        \n",
    "        # Buscar nombre del proyecto\n",
    "        if \"conservaci√≥n\" in text_lower and \"caminos\" in text_lower:\n",
    "            if \"comunidades ind√≠genas\" in text_lower:\n",
    "                info[\"nombre\"] = \"Conservaci√≥n de caminos de acceso a comunidades ind√≠genas\"\n",
    "            else:\n",
    "                info[\"nombre\"] = \"Conservaci√≥n de caminos\"\n",
    "        \n",
    "        # Buscar regi√≥n\n",
    "        region_match = re.search(r'regi√≥n\\s+de\\s+([^,\\n.]+)', text_lower)\n",
    "        if region_match:\n",
    "            info[\"region\"] = region_match.group(1).strip().title()\n",
    "        elif \"los r√≠os\" in text_lower:\n",
    "            info[\"region\"] = \"Los R√≠os\"\n",
    "        \n",
    "        # Buscar comunas\n",
    "        comunas_patterns = [\n",
    "            r'comunas?\\s+de\\s+([^,\\n.]+)',\n",
    "            r'lago\\s+ranco',\n",
    "            r'futrono',\n",
    "            r'valdivia'\n",
    "        ]\n",
    "        \n",
    "        for pattern in comunas_patterns:\n",
    "            matches = re.findall(pattern, text_lower)\n",
    "            for match in matches:\n",
    "                comuna = match.strip().title()\n",
    "                if comuna and comuna not in info[\"comunas\"]:\n",
    "                    info[\"comunas\"].append(comuna)\n",
    "        \n",
    "        # Buscar tipo de obra\n",
    "        if \"conservaci√≥n\" in text_lower:\n",
    "            info[\"tipo_obra\"] = \"Conservaci√≥n\"\n",
    "        elif \"construcci√≥n\" in text_lower:\n",
    "            info[\"tipo_obra\"] = \"Construcci√≥n\"\n",
    "        elif \"mejoramiento\" in text_lower:\n",
    "            info[\"tipo_obra\"] = \"Mejoramiento\"\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def _calculate_confidence(self, doc_type: str, codigos_count: int, proyecto_info: Dict) -> float:\n",
    "        \"\"\"Calcula la confianza de la detecci√≥n.\"\"\"\n",
    "        confidence = 0.5  # Base\n",
    "        \n",
    "        # Bonus por tipo de documento claro\n",
    "        if doc_type != \"documento_mop\":\n",
    "            confidence += 0.2\n",
    "        \n",
    "        # Bonus por c√≥digos MOP encontrados\n",
    "        if codigos_count > 0:\n",
    "            confidence += min(0.3, codigos_count * 0.05)\n",
    "        \n",
    "        # Bonus por informaci√≥n del proyecto\n",
    "        if proyecto_info.get(\"nombre\"):\n",
    "            confidence += 0.2\n",
    "        if proyecto_info.get(\"region\"):\n",
    "            confidence += 0.1\n",
    "        if proyecto_info.get(\"comunas\"):\n",
    "            confidence += 0.1\n",
    "        \n",
    "        return min(1.0, confidence)\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES PRINCIPALES OPTIMIZADAS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_mop_budgets_optimized():\n",
    "    \"\"\"\n",
    "    Analiza todos los documentos MOP con rate limiting optimizado.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ AN√ÅLISIS OPTIMIZADO DE PRESUPUESTOS MOP\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Buscar archivos de texto\n",
    "    text_files = list(RESULTS_DIR.glob(\"*_texto.txt\"))\n",
    "    \n",
    "    if not text_files:\n",
    "        print(\"‚ùå No hay archivos de texto para analizar\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüìö Archivos encontrados: {len(text_files)}\")\n",
    "    for f in text_files:\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"   - {f.name} ({size_kb:.1f} KB)\")\n",
    "    \n",
    "    # Confirmar procesamiento\n",
    "    response = input(f\"\\n¬øProceder con an√°lisis de {len(text_files)} archivos? (s/n): \")\n",
    "    if response.lower() != 's':\n",
    "        print(\"‚ùå An√°lisis cancelado\")\n",
    "        return None\n",
    "    \n",
    "    # Inicializar analizador optimizado\n",
    "    analyzer = MOPBudgetAnalyzerOptimized(client)\n",
    "    \n",
    "    # An√°lisis con delays autom√°ticos\n",
    "    results = analyzer.analyze_batch_with_delays(text_files)\n",
    "    \n",
    "    # Crear tabla resumen mejorada\n",
    "    summary_data = []\n",
    "    for r in results:\n",
    "        if r['success']:\n",
    "            analysis = r['analysis']\n",
    "            proyecto = analysis.get('proyecto', {})\n",
    "            presupuesto = analysis.get('presupuesto', {})\n",
    "            metadata = analysis.get('metadata', {})\n",
    "            \n",
    "            # Determinar si tiene datos presupuestarios\n",
    "            tiene_presupuesto = presupuesto.get('tiene_datos_presupuestarios', False)\n",
    "            tipo_doc = analysis.get('tipo_documento', 'documento_mop')\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Archivo': r['file'],\n",
    "                'Tipo Documento': tipo_doc.replace('_', ' ').title(),\n",
    "                'Proyecto': proyecto.get('nombre', 'N/D')[:60],\n",
    "                'Regi√≥n': proyecto.get('region', 'N/D'),\n",
    "                'Comunas': ', '.join(proyecto.get('comunas', ['N/D'])[:2]),\n",
    "                'Items': len(analysis.get('items', [])),\n",
    "                'Total Presupuesto': f\"${presupuesto.get('total_con_iva', 0):,.0f}\" if tiene_presupuesto else 'Sin datos',\n",
    "                'Tiene Presupuesto': 'S√≠' if tiene_presupuesto else 'No',\n",
    "                'Confianza': f\"{metadata.get('confianza_extraccion', metadata.get('confianza', 0))*100:.1f}%\",\n",
    "                'M√©todo': 'Fallback' if metadata.get('es_fallback') else 'Claude',\n",
    "                'Costo': f\"${r['cost']:.4f}\",\n",
    "                'Tiempo': f\"{r['time']:.1f}s\"\n",
    "            })\n",
    "        else:\n",
    "            error_type = r.get('error_type', 'unknown')\n",
    "            summary_data.append({\n",
    "                'Archivo': r['file'],\n",
    "                'Tipo Documento': 'ERROR',\n",
    "                'Proyecto': f'Error: {error_type}',\n",
    "                'Regi√≥n': 'N/A',\n",
    "                'Comunas': 'N/A',\n",
    "                'Items': 0,\n",
    "                'Total Presupuesto': '$0',\n",
    "                'Tiene Presupuesto': 'No',\n",
    "                'Confianza': '0%',\n",
    "                'M√©todo': 'Error',\n",
    "                'Costo': '$0',\n",
    "                'Tiempo': '0s'\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        df_summary = pd.DataFrame(summary_data)\n",
    "        display(df_summary)\n",
    "        \n",
    "        # Guardar Excel con resumen\n",
    "        excel_file = RESULTS_DIR / f\"analisis_optimized_resumen_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx\"\n",
    "        df_summary.to_excel(excel_file, index=False)\n",
    "        print(f\"\\nüíæ Resumen guardado en: {excel_file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_single_optimized(filename: str):\n",
    "    \"\"\"\n",
    "    Analiza un √∫nico documento con optimizaciones.\n",
    "    \"\"\"\n",
    "    text_file = RESULTS_DIR / filename\n",
    "    \n",
    "    if not text_file.exists():\n",
    "        print(f\"‚ùå Archivo no encontrado: {filename}\")\n",
    "        return None\n",
    "    \n",
    "    analyzer = MOPBudgetAnalyzerOptimized(client)\n",
    "    result = analyzer.analyze_document_optimized(text_file)\n",
    "    \n",
    "    if result['success']:\n",
    "        # Generar y mostrar reporte HTML\n",
    "        html_report = analyzer.generate_simple_html_report(result['analysis'])\n",
    "        display(HTML(html_report))\n",
    "        \n",
    "        # Guardar HTML\n",
    "        html_file = RESULTS_DIR / f\"{text_file.stem}_reporte_optimized.html\"\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_report)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Reporte HTML guardado: {html_file}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def analyze_mop_budgets_smart():\n",
    "    \"\"\"\n",
    "    An√°lisis inteligente que primero identifica el tipo de documento.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ AN√ÅLISIS INTELIGENTE DE DOCUMENTOS MOP\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Buscar archivos de texto\n",
    "    text_files = list(RESULTS_DIR.glob(\"*_texto.txt\"))\n",
    "    \n",
    "    if not text_files:\n",
    "        print(\"‚ùå No hay archivos de texto para analizar\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüìö Archivos encontrados: {len(text_files)}\")\n",
    "    \n",
    "    # Inicializar analizador\n",
    "    analyzer = MOPBudgetAnalyzerOptimized(client)\n",
    "    \n",
    "    # An√°lisis r√°pido primero\n",
    "    print(\"\\nüîç FASE 1: An√°lisis r√°pido de documentos\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    quick_results = []\n",
    "    for text_file in text_files:\n",
    "        with open(text_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        quick_analysis = analyzer.quick_document_analysis(text, text_file.name)\n",
    "        quick_results.append({\n",
    "            'file': text_file,\n",
    "            'analysis': quick_analysis\n",
    "        })\n",
    "        \n",
    "        tipo = quick_analysis['tipo_documento']\n",
    "        confianza = quick_analysis['confianza_deteccion']\n",
    "        codigos = quick_analysis['codigos_mop_encontrados']\n",
    "        \n",
    "        print(f\"   üìÑ {text_file.name}\")\n",
    "        print(f\"      Tipo: {tipo.replace('_', ' ').title()}\")\n",
    "        print(f\"      C√≥digos MOP: {codigos}\")\n",
    "        print(f\"      Confianza: {confianza*100:.1f}%\")\n",
    "        print(f\"      Proyecto: {quick_analysis['proyecto_detectado'].get('nombre', 'No identificado')[:50]}\")\n",
    "    \n",
    "    # Mostrar resumen de an√°lisis r√°pido\n",
    "    print(f\"\\nüìä RESUMEN AN√ÅLISIS R√ÅPIDO:\")\n",
    "    tipos_doc = {}\n",
    "    for qr in quick_results:\n",
    "        tipo = qr['analysis']['tipo_documento']\n",
    "        tipos_doc[tipo] = tipos_doc.get(tipo, 0) + 1\n",
    "    \n",
    "    for tipo, count in tipos_doc.items():\n",
    "        print(f\"   - {tipo.replace('_', ' ').title()}: {count} archivo(s)\")\n",
    "    \n",
    "    # Preguntar si continuar con an√°lisis completo\n",
    "    response = input(f\"\\n¬øContinuar con an√°lisis completo de {len(text_files)} archivos? (s/n): \")\n",
    "    if response.lower() != 's':\n",
    "        print(\"‚ùå An√°lisis cancelado\")\n",
    "        return quick_results\n",
    "    \n",
    "    # FASE 2: An√°lisis completo con Claude\n",
    "    print(f\"\\nü§ñ FASE 2: An√°lisis completo con Claude\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = analyzer.analyze_batch_with_delays([qr['file'] for qr in quick_results])\n",
    "    \n",
    "    # Combinar resultados\n",
    "    combined_results = []\n",
    "    for i, result in enumerate(results):\n",
    "        combined_result = result.copy()\n",
    "        combined_result['quick_analysis'] = quick_results[i]['analysis']\n",
    "        combined_results.append(combined_result)\n",
    "    \n",
    "    return combined_results\n",
    "\n",
    "def analyze_single_smart(filename: str):\n",
    "    \"\"\"\n",
    "    An√°lisis inteligente de un solo documento.\n",
    "    \"\"\"\n",
    "    text_file = RESULTS_DIR / filename\n",
    "    \n",
    "    if not text_file.exists():\n",
    "        print(f\"‚ùå Archivo no encontrado: {filename}\")\n",
    "        return None\n",
    "    \n",
    "    analyzer = MOPBudgetAnalyzerOptimized(client)\n",
    "    \n",
    "    # An√°lisis r√°pido primero\n",
    "    print(\"üîç An√°lisis r√°pido...\")\n",
    "    with open(text_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    quick_analysis = analyzer.quick_document_analysis(text, filename)\n",
    "    \n",
    "    print(f\"üìÑ Tipo de documento: {quick_analysis['tipo_documento'].replace('_', ' ').title()}\")\n",
    "    print(f\"üéØ C√≥digos MOP encontrados: {quick_analysis['codigos_mop_encontrados']}\")\n",
    "    print(f\"üìä Confianza: {quick_analysis['confianza_deteccion']*100:.1f}%\")\n",
    "    \n",
    "    proyecto = quick_analysis['proyecto_detectado']\n",
    "    if proyecto.get('nombre'):\n",
    "        print(f\"üèóÔ∏è Proyecto: {proyecto['nombre']}\")\n",
    "        print(f\"üìç Regi√≥n: {proyecto.get('region', 'No especificada')}\")\n",
    "        if proyecto.get('comunas'):\n",
    "            print(f\"üèòÔ∏è Comunas: {', '.join(proyecto['comunas'])}\")\n",
    "    \n",
    "    # An√°lisis completo\n",
    "    print(f\"\\nü§ñ An√°lisis completo con Claude...\")\n",
    "    result = analyzer.analyze_document_optimized(text_file)\n",
    "    \n",
    "    if result['success']:\n",
    "        # Mostrar reporte\n",
    "        html_report = analyzer.generate_simple_html_report(result['analysis'])\n",
    "        display(HTML(html_report))\n",
    "        \n",
    "        # Guardar HTML\n",
    "        html_file = RESULTS_DIR / f\"{text_file.stem}_reporte_smart.html\"\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_report)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Reporte HTML guardado: {html_file}\")\n",
    "    \n",
    "    # Combinar resultados\n",
    "    result['quick_analysis'] = quick_analysis\n",
    "    return result\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ BUDGET ANALYZER MOP INTELIGENTE CARGADO\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFunciones principales:\")\n",
    "print(\"  ‚Ä¢ analyze_mop_budgets_smart() - An√°lisis inteligente con detecci√≥n de tipo\")\n",
    "print(\"  ‚Ä¢ analyze_single_smart('bases1_texto.txt') - An√°lisis individual inteligente\")\n",
    "print(\"\\nFunciones optimizadas:\")\n",
    "print(\"  ‚Ä¢ analyze_mop_budgets_optimized() - An√°lisis con rate limiting\")\n",
    "print(\"  ‚Ä¢ analyze_single_optimized('archivo.txt') - An√°lisis individual optimizado\")\n",
    "print(\"\\nüß† CARACTER√çSTICAS INTELIGENTES:\")\n",
    "print(\"   - Detecci√≥n autom√°tica del tipo de documento\")\n",
    "print(\"   - An√°lisis r√°pido previo antes del an√°lisis completo\")\n",
    "print(\"   - Extracci√≥n mejorada de informaci√≥n del proyecto\")\n",
    "print(\"   - Manejo espec√≠fico seg√∫n tipo de documento\")\n",
    "print(\"   - Rate limiting autom√°tico\")\n",
    "print(\"\\nüí° Recomendado: results = analyze_mop_budgets_smart()\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca663fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ AN√ÅLISIS DE PRESUPUESTOS MOP CON CLAUDE SONNET 4\n",
      "================================================================================\n",
      "\n",
      "üìö Archivos a analizar: 3\n",
      "   - bases2_texto.txt (337.1 KB)\n",
      "   - bases3_texto.txt (306.5 KB)\n",
      "   - bases1_texto.txt (200.4 KB)\n",
      "\n",
      "ü§ñ Analizando con Claude Sonnet 4: bases2_texto.txt\n",
      "============================================================\n",
      "üìÑ Caracteres: 338,247\n",
      "üéØ Tokens estimados: 84,562\n",
      "‚è≥ Procesando con Claude...\n",
      "‚úÖ An√°lisis completado\n",
      "   ‚è±Ô∏è Tiempo: 14.1s\n",
      "   üí∞ Costo: $0.0866\n",
      "   üìä Items extra√≠dos: 0\n",
      "   üíæ Guardado: bases2_texto_analisis_budget.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto;\">\n",
       "            <h1 style=\"color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px;\">\n",
       "                üìä An√°lisis Presupuestario MOP\n",
       "            </h1>\n",
       "\n",
       "            <div style=\"background: #ecf0f1; padding: 15px; border-radius: 5px; margin: 20px 0;\">\n",
       "                <h2>üìã Informaci√≥n del Proyecto</h2>\n",
       "                <table style=\"width: 100%; background: white; padding: 10px;\">\n",
       "                    <tr>\n",
       "                        <td><strong>Nombre:</strong></td>\n",
       "                        <td>No se identifica proyecto espec√≠fico en el documento</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td><strong>Tipo:</strong></td>\n",
       "                        <td>Bases administrativas para contratos de obras p√∫blicas</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td><strong>Regi√≥n:</strong></td>\n",
       "                        <td>Los R√≠os</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td><strong>Comunas:</strong></td>\n",
       "                        <td>Valdivia</td>\n",
       "                    </tr>\n",
       "                </table>\n",
       "            </div>\n",
       "\n",
       "            <div style=\"background: #e8f8f5; padding: 15px; border-radius: 5px; margin: 20px 0;\">\n",
       "                <h2>üí∞ Presupuesto Total</h2>\n",
       "                <table style=\"width: 100%; background: white; padding: 10px;\">\n",
       "                    <tr>\n",
       "                        <td><strong>Total Neto:</strong></td>\n",
       "                        <td style=\"text-align: right; font-size: 1.2em;\">\n",
       "                            $0 CLP\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td><strong>IVA (19%):</strong></td>\n",
       "                        <td style=\"text-align: right;\">\n",
       "                            $0 CLP\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr style=\"border-top: 2px solid #3498db;\">\n",
       "                        <td><strong>Total con IVA:</strong></td>\n",
       "                        <td style=\"text-align: right; font-size: 1.3em; color: #27ae60;\">\n",
       "                            <strong>$0 CLP</strong>\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                </table>\n",
       "            </div>\n",
       "\n",
       "            <div style=\"margin: 20px 0;\">\n",
       "                <h2>üìù Items del Presupuesto (0 items)</h2>\n",
       "                <div style=\"max-height: 400px; overflow-y: auto;\">\n",
       "                    <table style=\"width: 100%; border-collapse: collapse;\">\n",
       "                        <thead style=\"background: #34495e; color: white; position: sticky; top: 0;\">\n",
       "                            <tr>\n",
       "                                <th style=\"padding: 10px; text-align: left;\">C√≥digo MOP</th>\n",
       "                                <th style=\"padding: 10px; text-align: left;\">Designaci√≥n</th>\n",
       "                                <th style=\"padding: 10px; text-align: center;\">Unidad</th>\n",
       "                                <th style=\"padding: 10px; text-align: right;\">Cantidad</th>\n",
       "                                <th style=\"padding: 10px; text-align: right;\">P.Unitario</th>\n",
       "                                <th style=\"padding: 10px; text-align: right;\">Total</th>\n",
       "                            </tr>\n",
       "                        </thead>\n",
       "                        <tbody>\n",
       "        \n",
       "                        </tbody>\n",
       "                    </table>\n",
       "                </div>\n",
       "            </div>\n",
       "\n",
       "            <div style=\"background: #fff3cd; padding: 15px; border-radius: 5px; margin: 20px 0;\">\n",
       "                <h3>‚úÖ Validaci√≥n del Presupuesto</h3>\n",
       "        \n",
       "                <p>Total Esperado: <strong>$718,998,624</strong></p>\n",
       "                <p>Diferencia: <strong>$718,998,624</strong> \n",
       "                   (100.00%)</p>\n",
       "                <p style=\"color: #dc3545; font-weight: bold;\">\n",
       "                    Estado: ‚ùå REVISAR\n",
       "                </p>\n",
       "            \n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Analizando con Claude Sonnet 4: bases3_texto.txt\n",
      "============================================================\n",
      "üìÑ Caracteres: 308,486\n",
      "üéØ Tokens estimados: 77,122\n",
      "‚è≥ Procesando con Claude...\n",
      "‚úÖ An√°lisis completado\n",
      "   ‚è±Ô∏è Tiempo: 14.7s\n",
      "   üí∞ Costo: $0.0868\n",
      "   üìä Items extra√≠dos: 0\n",
      "   üíæ Guardado: bases3_texto_analisis_budget.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto;\">\n",
       "            <h1 style=\"color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px;\">\n",
       "                üìä An√°lisis Presupuestario MOP\n",
       "            </h1>\n",
       "\n",
       "            <div style=\"background: #ecf0f1; padding: 15px; border-radius: 5px; margin: 20px 0;\">\n",
       "                <h2>üìã Informaci√≥n del Proyecto</h2>\n",
       "                <table style=\"width: 100%; background: white; padding: 10px;\">\n",
       "                    <tr>\n",
       "                        <td><strong>Nombre:</strong></td>\n",
       "                        <td>No se identifica proyecto espec√≠fico en el documento</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td><strong>Tipo:</strong></td>\n",
       "                        <td>Especificaciones de Participaci√≥n Ciudadana y Gesti√≥n de Calidad</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td><strong>Regi√≥n:</strong></td>\n",
       "                        <td>Regi√≥n de Los R√≠os</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td><strong>Comunas:</strong></td>\n",
       "                        <td></td>\n",
       "                    </tr>\n",
       "                </table>\n",
       "            </div>\n",
       "\n",
       "            <div style=\"background: #e8f8f5; padding: 15px; border-radius: 5px; margin: 20px 0;\">\n",
       "                <h2>üí∞ Presupuesto Total</h2>\n",
       "                <table style=\"width: 100%; background: white; padding: 10px;\">\n",
       "                    <tr>\n",
       "                        <td><strong>Total Neto:</strong></td>\n",
       "                        <td style=\"text-align: right; font-size: 1.2em;\">\n",
       "                            $0 CLP\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td><strong>IVA (19%):</strong></td>\n",
       "                        <td style=\"text-align: right;\">\n",
       "                            $0 CLP\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr style=\"border-top: 2px solid #3498db;\">\n",
       "                        <td><strong>Total con IVA:</strong></td>\n",
       "                        <td style=\"text-align: right; font-size: 1.3em; color: #27ae60;\">\n",
       "                            <strong>$0 CLP</strong>\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                </table>\n",
       "            </div>\n",
       "\n",
       "            <div style=\"margin: 20px 0;\">\n",
       "                <h2>üìù Items del Presupuesto (0 items)</h2>\n",
       "                <div style=\"max-height: 400px; overflow-y: auto;\">\n",
       "                    <table style=\"width: 100%; border-collapse: collapse;\">\n",
       "                        <thead style=\"background: #34495e; color: white; position: sticky; top: 0;\">\n",
       "                            <tr>\n",
       "                                <th style=\"padding: 10px; text-align: left;\">C√≥digo MOP</th>\n",
       "                                <th style=\"padding: 10px; text-align: left;\">Designaci√≥n</th>\n",
       "                                <th style=\"padding: 10px; text-align: center;\">Unidad</th>\n",
       "                                <th style=\"padding: 10px; text-align: right;\">Cantidad</th>\n",
       "                                <th style=\"padding: 10px; text-align: right;\">P.Unitario</th>\n",
       "                                <th style=\"padding: 10px; text-align: right;\">Total</th>\n",
       "                            </tr>\n",
       "                        </thead>\n",
       "                        <tbody>\n",
       "        \n",
       "                        </tbody>\n",
       "                    </table>\n",
       "                </div>\n",
       "            </div>\n",
       "\n",
       "            <div style=\"background: #fff3cd; padding: 15px; border-radius: 5px; margin: 20px 0;\">\n",
       "                <h3>‚úÖ Validaci√≥n del Presupuesto</h3>\n",
       "        \n",
       "                <p>Total Esperado: <strong>$718,998,624</strong></p>\n",
       "                <p>Diferencia: <strong>$718,998,624</strong> \n",
       "                   (100.00%)</p>\n",
       "                <p style=\"color: #dc3545; font-weight: bold;\">\n",
       "                    Estado: ‚ùå REVISAR\n",
       "                </p>\n",
       "            \n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Analizando con Claude Sonnet 4: bases1_texto.txt\n",
      "============================================================\n",
      "üìÑ Caracteres: 200,890\n",
      "üéØ Tokens estimados: 50,222\n",
      "‚è≥ Procesando con Claude...\n",
      "‚ùå Error: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'This request would exceed the rate limit for your organization (11cfa296-d5b1-45da-a861-e519afa2f730) of 30,000 input tokens per minute. For details, refer to: https://docs.anthropic.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}, 'request_id': 'req_011CSqe8GMRi89v2Z6r9giKp'}\n",
      "\n",
      "================================================================================\n",
      "üìä RESUMEN DE AN√ÅLISIS\n",
      "================================================================================\n",
      "‚úÖ Archivos procesados: 3\n",
      "‚è±Ô∏è Tiempo total: 28.8s\n",
      "üí∞ Costo total: $0.1734 USD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Archivo</th>\n",
       "      <th>Proyecto</th>\n",
       "      <th>Items</th>\n",
       "      <th>Total Presupuesto</th>\n",
       "      <th>Validaci√≥n</th>\n",
       "      <th>Costo An√°lisis</th>\n",
       "      <th>Tiempo (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bases2_texto.txt</td>\n",
       "      <td>No se identifica proyecto espec√≠fico en el doc...</td>\n",
       "      <td>0</td>\n",
       "      <td>$0</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>$0.0866</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bases3_texto.txt</td>\n",
       "      <td>No se identifica proyecto espec√≠fico en el doc...</td>\n",
       "      <td>0</td>\n",
       "      <td>$0</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>$0.0868</td>\n",
       "      <td>14.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Archivo                                           Proyecto  Items  \\\n",
       "0  bases2_texto.txt  No se identifica proyecto espec√≠fico en el doc...      0   \n",
       "1  bases3_texto.txt  No se identifica proyecto espec√≠fico en el doc...      0   \n",
       "\n",
       "  Total Presupuesto Validaci√≥n Costo An√°lisis Tiempo (s)  \n",
       "0                $0          ‚ùå        $0.0866       14.1  \n",
       "1                $0          ‚ùå        $0.0868       14.7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Resumen guardado en: storage/projects/conservacion_caminos/results/analisis_budget_resumen_20250905_1322.xlsx\n"
     ]
    }
   ],
   "source": [
    "results = analyze_mop_budgets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b808b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç An√°lisis r√°pido...\n",
      "üìÑ Tipo de documento: Presupuesto\n",
      "üéØ C√≥digos MOP encontrados: 1\n",
      "üìä Confianza: 100.0%\n",
      "üèóÔ∏è Proyecto: Conservaci√≥n de caminos de acceso a comunidades ind√≠genas\n",
      "üìç Regi√≥n: Los R√≠os Valdivia 2024 Yungay N¬∫ 621\n",
      "üèòÔ∏è Comunas: Lago Ranco Y Futrono, Lago Ranco Y Futrono Con Una Longitud Total De 7, Lago Ranco, Futrono, Valdivia\n",
      "\n",
      "ü§ñ An√°lisis completo con Claude...\n",
      "\n",
      "ü§ñ Analizando con Claude Sonnet 4 (optimizado): bases1_texto.txt\n",
      "======================================================================\n",
      "üìÑ Caracteres: 200,890\n",
      "üéØ Tokens estimados (limitados): 20,000\n",
      "‚è≥ Procesando con Claude...\n",
      "‚úÖ An√°lisis completado\n",
      "   ‚è±Ô∏è Tiempo: 44.8s\n",
      "   üí∞ Costo: $0.0723\n",
      "   üìä Items extra√≠dos: 15\n",
      "   üíæ Guardado: bases1_texto_analisis_optimized.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <title>An√°lisis de Bases Administrativas - Conservaci√≥n de Caminos de Acceso a Comunidades Ind√≠genas Etapa XII, Comunas de Lago Ranco y Futrono, Provincia del Ranco, Regi√≥n de Los R√≠os</title>\n",
       "    <style>\n",
       "        body { font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }\n",
       "        .header { background: #2c3e50; color: white; padding: 20px; border-radius: 5px; margin-bottom: 20px; }\n",
       "        .section { margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; background: #f8f9fa; }\n",
       "        .presupuesto { background: #e8f5e8; }\n",
       "        .especificaciones { background: #e8f0ff; }\n",
       "        .warning { background: #fff3cd; border-color: #ffeaa7; }\n",
       "        .total { font-size: 1.5em; color: #27ae60; font-weight: bold; }\n",
       "        .no-presupuesto { font-size: 1.2em; color: #e67e22; font-weight: bold; }\n",
       "        table { width: 100%; border-collapse: collapse; margin-top: 10px; }\n",
       "        th, td { padding: 8px; border: 1px solid #ddd; text-align: left; }\n",
       "        th { background: #f8f9fa; font-weight: bold; }\n",
       "        .items-table { max-height: 400px; overflow-y: auto; }\n",
       "        .badge { display: inline-block; padding: 3px 8px; border-radius: 3px; font-size: 0.9em; }\n",
       "        .badge-success { background: #d4edda; color: #155724; }\n",
       "        .badge-warning { background: #fff3cd; color: #856404; }\n",
       "        .badge-info { background: #d1ecf1; color: #0c5460; }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "    <div class=\"header\">\n",
       "        <h1>üìä An√°lisis de Bases Administrativas</h1>\n",
       "        <p><strong>Conservaci√≥n de Caminos de Acceso a Comunidades Ind√≠genas Etapa XII, Comunas de Lago Ranco y Futrono, Provincia del Ranco, Regi√≥n de Los R√≠os</strong></p>\n",
       "        <span class=\"badge badge-info\">Tipo: Bases Administrativas</span>\n",
       "    </div>\n",
       "\n",
       "    <div class=\"section\">\n",
       "        <h2>üìç Informaci√≥n del Proyecto</h2>\n",
       "        <table>\n",
       "            <tr><td><strong>Nombre Completo:</strong></td><td>Conservaci√≥n de Caminos de Acceso a Comunidades Ind√≠genas Etapa XII, Comunas de Lago Ranco y Futrono, Provincia del Ranco, Regi√≥n de Los R√≠os</td></tr>\n",
       "            <tr><td><strong>Regi√≥n:</strong></td><td>De Los R√≠os</td></tr>\n",
       "            <tr><td><strong>Provincia:</strong></td><td>Del Ranco</td></tr>\n",
       "            <tr><td><strong>Comunas:</strong></td><td>Lago Ranco, Futrono</td></tr>\n",
       "            <tr><td><strong>Tipo de Obra:</strong></td><td>Conservaci√≥n de caminos de acceso a comunidades ind√≠genas</td></tr>\n",
       "            <tr><td><strong>Etapa:</strong></td><td>XII</td></tr>\n",
       "            <tr><td><strong>Mandante:</strong></td><td>Direcci√≥n de Vialidad - Ministerio de Obras P√∫blicas</td></tr>\n",
       "        </table>\n",
       "    </div>\n",
       "    <div class=\"section presupuesto\">\n",
       "        <h2>üí∞ Informaci√≥n Presupuestaria</h2>\n",
       "        <p class=\"total\">Total Identificado: $604,200,524 CLP</p>\n",
       "        <table>\n",
       "            <tr><td><strong>Total Neto:</strong></td><td>$604,200,524 CLP</td></tr>\n",
       "            <tr><td><strong>IVA (19%):</strong></td><td>$114,798,100 CLP</td></tr>\n",
       "            <tr><td><strong>Total con IVA:</strong></td><td>$604,200,524 CLP</td></tr>\n",
       "        </table>\n",
       "    </div>\n",
       "\n",
       "    <div class=\"section\">\n",
       "        <h2>üìù Items Presupuestarios (15 items)</h2>\n",
       "        <div class=\"items-table\">\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <th>C√≥digo MOP</th>\n",
       "                        <th>Descripci√≥n</th>\n",
       "                        <th>Cantidad</th>\n",
       "                        <th>Unidad</th>\n",
       "                        <th>P.Unitario</th>\n",
       "                        <th>Total</th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    <tr>\n",
       "                        <td>7.301.1d</td>\n",
       "                        <td>Limpieza Manual de la Faja...</td>\n",
       "                        <td>3.14</td>\n",
       "                        <td>Km</td>\n",
       "                        <td>$988,596</td>\n",
       "                        <td>$3,108,146</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.302.5d</td>\n",
       "                        <td>Terraplenes, Tm√°x Bajo 4\"...</td>\n",
       "                        <td>7,706.70</td>\n",
       "                        <td>m3</td>\n",
       "                        <td>$28,543</td>\n",
       "                        <td>$219,972,338</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.302.5e</td>\n",
       "                        <td>Conformacion de La Plataforma...</td>\n",
       "                        <td>24,555.00</td>\n",
       "                        <td>m2</td>\n",
       "                        <td>$2,392</td>\n",
       "                        <td>$58,735,560</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.302.7a</td>\n",
       "                        <td>Excavaci√≥n en Terreno de Cualquier Naturaleza...</td>\n",
       "                        <td>8,800.90</td>\n",
       "                        <td>m3</td>\n",
       "                        <td>$9,461</td>\n",
       "                        <td>$83,265,315</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.303.13d1</td>\n",
       "                        <td>Alcantarillas de Tubos de Polietileno de Alta Densidad Estru...</td>\n",
       "                        <td>85.00</td>\n",
       "                        <td>m</td>\n",
       "                        <td>$214,789</td>\n",
       "                        <td>$18,257,065</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.303.13d2</td>\n",
       "                        <td>Alcantarillas de Tubos de Polietileno de Alta Densidad Estru...</td>\n",
       "                        <td>18.00</td>\n",
       "                        <td>m</td>\n",
       "                        <td>$300,628</td>\n",
       "                        <td>$5,411,304</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.303.13d3</td>\n",
       "                        <td>Alcantarillas de Tubos de Polietileno de Alta Densidad Estru...</td>\n",
       "                        <td>24.00</td>\n",
       "                        <td>m</td>\n",
       "                        <td>$236,894</td>\n",
       "                        <td>$5,685,456</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.303.13d5</td>\n",
       "                        <td>Alcantarillas de Tubos de Polietileno de Alta Densidad Estru...</td>\n",
       "                        <td>21.00</td>\n",
       "                        <td>m</td>\n",
       "                        <td>$368,691</td>\n",
       "                        <td>$7,742,511</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.303.17b</td>\n",
       "                        <td>Construcci√≥n de Fosos y Contrafosos en Terreno de Cualquier ...</td>\n",
       "                        <td>671.00</td>\n",
       "                        <td>m</td>\n",
       "                        <td>$4,997</td>\n",
       "                        <td>$3,352,987</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.306.4a</td>\n",
       "                        <td>Recebo de Capas de Rodadura Granulares, T√°ma√±o M√°ximo 1 1/2\"...</td>\n",
       "                        <td>4,544.80</td>\n",
       "                        <td>m3</td>\n",
       "                        <td>$30,837</td>\n",
       "                        <td>$140,147,998</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>ETE.1</td>\n",
       "                        <td>Huellas en Base a Losetas de Hormigon Armado...</td>\n",
       "                        <td>1,234.80</td>\n",
       "                        <td>m2</td>\n",
       "                        <td>$33,704</td>\n",
       "                        <td>$41,617,699</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.311.1</td>\n",
       "                        <td>Instalaci√≥n de Faena y Campamentos en Obras de Mantenimiento...</td>\n",
       "                        <td>1.00</td>\n",
       "                        <td>gl</td>\n",
       "                        <td>$4,898,209</td>\n",
       "                        <td>$4,898,209</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.311.2</td>\n",
       "                        <td>Apertura, Uso y Abandono de Botaderos en Obras de Mantenimie...</td>\n",
       "                        <td>1.00</td>\n",
       "                        <td>gl</td>\n",
       "                        <td>$4,898,209</td>\n",
       "                        <td>$4,898,209</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>7.311.3</td>\n",
       "                        <td>Apertura, Explotaci√≥n y Abandono de Empr√©stitos en Obras de ...</td>\n",
       "                        <td>1.00</td>\n",
       "                        <td>gl</td>\n",
       "                        <td>$5,510,485</td>\n",
       "                        <td>$5,510,485</td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>804-2</td>\n",
       "                        <td>Plan de Gesti√≥n de Residuos de Construccion Y/O Demoliciones...</td>\n",
       "                        <td>1.00</td>\n",
       "                        <td>gl</td>\n",
       "                        <td>$1,597,242</td>\n",
       "                        <td>$1,597,242</td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "    <div class=\"section especificaciones\">\n",
       "        <h2>üìã Especificaciones T√©cnicas Identificadas</h2>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td><strong>Participaci√≥n Ciudadana:</strong></td>\n",
       "                <td>‚úÖ S√≠</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td><strong>Gesti√≥n de Calidad:</strong></td>\n",
       "                <td>‚úÖ S√≠</td>\n",
       "            </tr>\n",
       "        </table>\n",
       "        <h3>Otras Especificaciones:</h3>\n",
       "        <ul><li>Plazo de ejecuci√≥n: 365 d√≠as corridos</li><li>Modalidad: Serie de Precios Unitarios</li><li>Requiere permisos de ingreso a predios de comunidades ind√≠genas</li><li>Incluye medidas de mitigaci√≥n ambiental</li><li>Personal m√≠nimo requerido: Profesional Residente, Encargado de Gesti√≥n de Calidad, Experto en Prevenci√≥n de Riesgos, Encargado de Laboratorio, Encargado de Topograf√≠a, Encargado de Medio Ambiente y Participaci√≥n Ciudadana</li><li>Licitaci√≥n electr√≥nica a trav√©s de mercadopublico.cl</li><li>11 caminos de acceso con longitud total de 7,38 km</li><li>Incluye trabajo con comunidades ind√≠genas bajo Convenio 169 OIT</li></ul></div>\n",
       "    <div class=\"section\">\n",
       "        <h3>‚ÑπÔ∏è Informaci√≥n del An√°lisis</h3>\n",
       "        <table>\n",
       "            <tr><td><strong>Fecha:</strong></td><td>05/09/2025 13:25</td></tr>\n",
       "            <tr><td><strong>Tipo de Documento:</strong></td><td>Bases Administrativas</td></tr>\n",
       "            <tr><td><strong>Items Extra√≠dos:</strong></td><td>15</td></tr>\n",
       "            <tr><td><strong>Confianza:</strong></td><td>98.0%</td></tr>\n",
       "            <tr><td><strong>M√©todo:</strong></td><td>An√°lisis Claude</td></tr>\n",
       "        </table><h4>Observaciones:</h4><ul><li>Documento completo de bases administrativas con presupuesto oficial incluido</li><li>Proyecto espec√≠fico para comunidades ind√≠genas con requisitos especiales de participaci√≥n ciudadana</li><li>Incluye coordenadas georreferenciadas de cada camino</li><li>Contiene informaci√≥n detallada de contactos de representantes de comunidades</li><li>C√≥digo SAFI: 392210, C√≥digo BIP: 40040161-0</li></ul>\n",
       "    </div>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Reporte HTML guardado: storage/projects/conservacion_caminos/results/bases1_texto_reporte_smart.html\n"
     ]
    }
   ],
   "source": [
    "result = analyze_single_smart('bases1_texto.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
